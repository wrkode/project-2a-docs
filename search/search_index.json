{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Mirantis Project 2A Docs","text":""},{"location":"#introduction","title":"Introduction","text":"<p>Mirantis Project 2A is focused on developing a consistent way to deploy  and manage Kubernetes clusters at scale. Think of Project 2A as a \"super  control plane\" designed to manage other Kubernetes control planes.  Whether  you want to manage Kubernetes clusters on-premises, in the cloud, or a combination of both, Project 2A provides a consistent way to do so.  With full life-cycle management, including provisioning, configuration, and maintenance, Project 2A is designed to be a repeatable and secure way  to manage your Kubernetes clusters in a central location.</p>"},{"location":"#project-2a-vs-hmc","title":"Project 2A vs HMC","text":"<p>Project 2A includes all of the components below, but because HMC was the first component to be developed, it is still referred to as \"HMC\" in some documentation. In many ways \"Project 2A\" and \"HMC\" are synonymous.</p> <p>2A is built around the creation of a set of standardised templates that enable  easy, repeatable cluster deployments and life cycle management. </p> <p>The main components of 2A include:</p> <ul> <li> <p>Hybrid Multi-Cluster Controller (HMC)</p> <p>Deployment and life-cycle management of Kubernetes clusters, including configuration, updates, and other CRUD operations.</p> </li> <li> <p>State Management Controller (SMC)</p> <p>Installation and life-cycle management of beach-head services, policy, Kubernetes API configurations and more.</p> </li> <li> <p>Observability (OBS)</p> <p>Cluster and beach-head services monitoring, events and log management.</p> </li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>See the 2A Quick Start Guide</p>"},{"location":"#supported-providers","title":"Supported Providers","text":"<p>Project 2A leverages the Cluster API provider ecosystem, the following providers have had <code>ProviderTemplates</code> created and validated, and more are in the works.</p> <ul> <li>AWS</li> <li>Azure</li> <li>vSphere</li> </ul>"},{"location":"#development-documentation","title":"Development Documentation","text":"<p>Documentation related to development process and developer specific notes located in the main repository.</p>"},{"location":"architecture/","title":"Architecture","text":"<p>Below is a diagram that provides an overview of how Project 2A works.</p>"},{"location":"architecture/#architectural-overview","title":"Architectural Overview","text":"<pre><code>---\ntitle: HMC Overview\n---\nerDiagram\n    USER ||--o{ HMC : uses\n    USER ||--o{ Template : assigns\n    Template ||--o{ HMC : \"used by\"\n    HMC ||--o{ CAPI : connects\n    CAPI ||--|{ CAPV : provider\n    CAPI ||--|{ CAPA : provider\n    CAPI ||--|{ CAPZ : provider\n    CAPI ||--|{ K0smotron : Bootstrap\n    K0smotron |o..o| CAPV : uses\n    K0smotron |o..o| CAPA : uses\n    K0smotron |o..o| CAPZ : uses</code></pre>"},{"location":"glossary/","title":"Glossary","text":"<p>This glossary is a collection of terms related to Project 2A. It clarifies some of the unique terms and concepts we use or explains more common ones that may need a little clarity in the way we use them.</p>"},{"location":"glossary/#beach-head-services","title":"Beach-head services","text":"<p>We use the term to refer to those Kubernetes services that need to be installed on a Kubernetes cluster to make it actually useful, for example: an ingress controller, CNI, and/or CSI. While from the perspective of how they are deployed they are no different from other Kubernetes services, we define them as distinct from the apps and services deployed as part of the applications.</p>"},{"location":"glossary/#cluster-api-capi","title":"Cluster API (CAPI)","text":"<p>CAPI is a Kubernetes project that provides a declarative way to manage the  lifecycle of Kubernetes clusters. It abstracts the underlying infrastructure,  allowing users to create, scale, upgrade, and delete clusters using a  consistent API. CAPI is extensible via providers that offer infrastructure-  specific functionality, such as AWS, Azure, and vSphere.</p>"},{"location":"glossary/#capi-provider-see-also-infrastructure-provider","title":"CAPI provider (see also Infrastructure provider)","text":"<p>A CAPI provider is a Kubernetes CAPI extension that allows 2A to manage and drive the creation of clusters on a specific infrastructure via API calls.</p>"},{"location":"glossary/#capa","title":"CAPA","text":"<p>CAPA stands for Cluster API Provider for AWS.</p>"},{"location":"glossary/#capv","title":"CAPV","text":"<p>CAPV stands for Cluster API Provider for vSphere.</p>"},{"location":"glossary/#capz","title":"CAPZ","text":"<p>CAPZ stands for Cluster API Provider for Azure.</p>"},{"location":"glossary/#cloud-controller-manager","title":"Cloud Controller Manager","text":"<p>Cloud Controller Manager (CCM) is a Kubernetes component that embeds logic to manage a specific infrastructure provider.</p>"},{"location":"glossary/#clusteridentity","title":"ClusterIdentity","text":"<p>ClusterIdentity is a Kubernetes object that references a Secret object containing credentials for a specific infrastructure provider.</p>"},{"location":"glossary/#credential","title":"Credential","text":"<p>A <code>Credential</code> is a custom resource (CR) in HMC that supplies 2A with the necessary credentials to manage a specific infrastructure. The credential object references other CRs with infrastructure-specific credentials such as access keys, passwords, certificates, etc. This means that a credential is specific to the CAPI provider that uses it.</p>"},{"location":"glossary/#hosted-control-plane-hcp","title":"Hosted Control Plane (HCP)","text":"<p>An HCP is a Kubernetes control plane that runs outside of the clusters it manages. Instead of running the control plane components (like the API server, controller manager, and etcd) within the same cluster as the worker nodes, the control plane is hosted on a separate, often centralized, infrastructure. This approach can provide benefits such as easier management, improved security, and better resource utilization, as the control plane can be scaled independently of the worker nodes.</p>"},{"location":"glossary/#infrastructure-provider-see-also-capi-provider","title":"Infrastructure provider (see also CAPI provider)","text":"<p>An infrastructure provider (aka <code>InfrastructureProvider</code>) is a Kubernetes custom resource (CR) that defines the infrastructure-specific configuration needed for managing Kubernetes clusters. It enables Cluster API (CAPI) to provision and manage clusters on a specific infrastructure platform (e.g., AWS, Azure, VMware, OpenStack, etc.).</p>"},{"location":"glossary/#managed-cluster","title":"Managed cluster","text":"<p>A Kubernetes cluster created and managed by Project 2A.</p>"},{"location":"glossary/#management-cluster","title":"Management cluster","text":"<p>The Kubernetes cluster where 2A is installed and from which all other managed clusters are managed.</p>"},{"location":"clustertemplates/aws/hosted-control-plane/","title":"AWS Hosted control plane deployment","text":"<p>This section covers setting up for a k0smotron hosted control plane on AWS.</p>"},{"location":"clustertemplates/aws/hosted-control-plane/#prerequisites","title":"Prerequisites","text":"<ul> <li>Management Kubernetes cluster (v1.28+) deployed on AWS with HMC installed on it</li> <li>Default storage class configured on the management cluster</li> <li>VPC ID for the worker nodes</li> <li>Subnet ID which will be used along with AZ information</li> <li>AMI ID which will be used to deploy worker nodes</li> </ul> <p>Keep in mind that all control plane components for all managed clusters will reside in the management cluster.</p>"},{"location":"clustertemplates/aws/hosted-control-plane/#networking","title":"Networking","text":"<p>The networking resources in AWS which are needed for a managed cluster can be reused with a management cluster.</p> <p>If you deployed your AWS Kubernetes cluster using Cluster API Provider AWS (CAPA) you can obtain all the necessary data with the commands below or use the template found below in the HMC ManagedCluster manifest generation section.</p> <p>If using the <code>aws-standalone-cp</code> template to deploy a hosted cluster it is recommended to use a <code>t3.large</code> or larger instance type as the <code>hmc-controller</code> and other provider controllers will need a large amount of resources to run.</p> <p>VPC ID</p> <pre><code>    kubectl get awscluster &lt;cluster-name&gt; -o go-template='{{.spec.network.vpc.id}}'\n</code></pre> <p>Subnet ID</p> <pre><code>    kubectl get awscluster &lt;cluster-name&gt; -o go-template='{{(index .spec.network.subnets 0).resourceID}}'\n</code></pre> <p>Availability zone</p> <pre><code>    kubectl get awscluster &lt;cluster-name&gt; -o go-template='{{(index .spec.network.subnets 0).availabilityZone}}'\n</code></pre> <p>Security group <pre><code>    kubectl get awscluster &lt;cluster-name&gt; -o go-template='{{.status.networkStatus.securityGroups.node.id}}'\n</code></pre></p> <p>AMI id</p> <pre><code>    kubectl get awsmachinetemplate &lt;cluster-name&gt;-worker-mt -o go-template='{{.spec.template.spec.ami.id}}'\n</code></pre> <p>If you want to use different VPCs/regions for your management or managed clusters you should setup additional connectivity rules like VPC peering.</p>"},{"location":"clustertemplates/aws/hosted-control-plane/#hmc-managedcluster-manifest","title":"HMC ManagedCluster manifest","text":"<p>With all the collected data your <code>ManagedCluster</code> manifest will look similar to this:</p> <pre><code>apiVersion: hmc.mirantis.com/v1alpha1\nkind: ManagedCluster\nmetadata:\n  name: aws-hosted-cp\nspec:\n  template: aws-hosted-cp\n  credential: aws-credential\n  config:\n    vpcID: vpc-0a000000000000000\n    region: us-west-1\n    publicIP: true\n    subnets:\n      - id: subnet-0aaaaaaaaaaaaaaaa\n        availabilityZone: us-west-1b\n    instanceType: t3.medium\n    securityGroupIDs:\n      - sg-0e000000000000000\n</code></pre> <p>Note</p> <p> In this example we're using the <code>us-west-1</code> region, but you should use the region of your VPC.</p>"},{"location":"clustertemplates/aws/hosted-control-plane/#hmc-managedcluster-manifest-generation","title":"HMC ManagedCluster manifest generation","text":"<p>Grab the following <code>ManagedCluster</code> manifest template and save it to a file named <code>managedcluster.yaml.tpl</code>:</p> <pre><code>apiVersion: hmc.mirantis.com/v1alpha1\nkind: ManagedCluster\nmetadata:\n  name: aws-hosted\nspec:\n  template: aws-hosted-cp\n  credential: aws-credential\n  config:\n    vpcID: \"{{.spec.network.vpc.id}}\"\n    region: \"{{.spec.region}}\"\n    subnets:\n      - id: \"{{(index .spec.network.subnets 0).resourceID}}\"\n        availabilityZone: \"{{(index .spec.network.subnets 0).availabilityZone}}\"\n    instanceType: t3.medium\n    securityGroupIDs:\n      - \"{{.status.networkStatus.securityGroups.node.id}}\"\n</code></pre> <p>Then run the following command to create the <code>managedcluster.yaml</code>:</p> <pre><code>kubectl get awscluster cluster -o go-template=\"$(cat managedcluster.yaml.tpl)\" &gt; managedcluster.yaml\n</code></pre>"},{"location":"clustertemplates/aws/hosted-control-plane/#deployment-tips","title":"Deployment Tips","text":"<ul> <li>Ensure HMC templates and the controller image are somewhere public and   fetchable.</li> <li>For installing the HMC charts and templates from a custom repository, load   the <code>kubeconfig</code> from the cluster and run the commands:</li> </ul> <p><pre><code>KUBECONFIG=kubeconfig IMG=\"ghcr.io/mirantis/hmc/controller-ci:v0.0.1-179-ga5bdf29\" REGISTRY_REPO=\"oci://ghcr.io/mirantis/hmc/charts-ci\" make dev-apply\nKUBECONFIG=kubeconfig make dev-templates\n</code></pre> * The infrastructure will need to manually be marked <code>Ready</code> to get the   <code>MachineDeployment</code> to scale up.  You can patch the <code>AWSCluster</code> kind using   the command:</p> <pre><code>KUBECONFIG=kubeconfig kubectl patch AWSCluster &lt;hosted-cluster-name&gt; --type=merge --subresource status --patch 'status: {ready: true}' -n hmc-system\n</code></pre> <p>For additional information on why this is required click here.</p>"},{"location":"clustertemplates/aws/template-parameters/","title":"AWS template parameters","text":""},{"location":"clustertemplates/aws/template-parameters/#aws-ami","title":"AWS AMI","text":"<p>By default AMI ID will be looked up automatically using the latest Amazon Linux 2 image.</p> <p>You can override lookup parameters to search your desired image automatically or use AMI ID directly. If both AMI ID and lookup parameters are defined AMI ID will have higher precedence.</p>"},{"location":"clustertemplates/aws/template-parameters/#image-lookup","title":"Image lookup","text":"<p>To configure automatic AMI lookup 3 parameters are used:</p> <ul> <li><code>.imageLookup.format</code> - used directly as value for the <code>name</code> filter (see the describe-images filters).</li> <li> <p>Supports substitutions for <code>{{.BaseOS}}</code> and <code>{{.K8sVersion}}</code> with the base OS and kubernetes version, respectively.</p> </li> <li> <p><code>.imageLookup.org</code> - AWS org ID which will be used as value for the <code>owner-id</code> filter.</p> </li> <li> <p><code>.imageLookup.baseOS</code> - will be used as value for <code>{{.BaseOS}}</code> substitution in the <code>.imageLookup.format</code> string.</p> </li> </ul>"},{"location":"clustertemplates/aws/template-parameters/#ami-id","title":"AMI ID","text":"<p>AMI ID can be directly used in the <code>.amiID</code> parameter.</p>"},{"location":"clustertemplates/aws/template-parameters/#capa-prebuilt-amis","title":"CAPA prebuilt AMIs","text":"<p>Use <code>clusterawsadm</code> to get available AMIs to deploy managed cluster:</p> <pre><code>clusterawsadm ami list\n</code></pre> <p>For details, see Pre-built Kubernetes AMIs.</p>"},{"location":"clustertemplates/aws/template-parameters/#ssh-access-to-cluster-nodes","title":"SSH access to cluster nodes","text":"<p>To access the nodes using the SSH protocol, several things should be configured:</p> <ul> <li>An SSH key added in the region where you want to deploy the cluster</li> <li>Bastion host is enabled</li> </ul>"},{"location":"clustertemplates/aws/template-parameters/#ssh-keys","title":"SSH keys","text":"<p>Only one SSH key is supported and it should be added in AWS prior to creating the <code>ManagedCluster</code> object. The name of the key should then be placed under <code>.spec.config.sshKeyName</code>.</p> <p>The same SSH key will be used for all machines and a bastion host.</p> <p>To enable bastion you should add <code>.spec.config.bastion.enabled</code> option in the <code>ManagedCluster</code> object to <code>true</code>.</p> <p>Full list of the bastion configuration options could be fould in CAPA docs.</p> <p>The resulting <code>ManagedCluster</code> can look like this:</p> <pre><code>apiVersion: hmc.mirantis.com/v1alpha1\nkind: ManagedCluster\nmetadata:\n  name: cluster-1\nspec:\n  template: aws-standalone-cp-0-0-2\n  credential: aws-cred\n  config:\n    sshKeyName: foobar\n    bastion:\n      enabled: true\n...\n</code></pre>"},{"location":"clustertemplates/aws/template-parameters/#eks-templates","title":"EKS templates","text":"<p>EKS templates use the parameters similar to AWS and resulting EKS <code>ManagedCluster</code> can look like this:</p> <pre><code>apiVersion: hmc.mirantis.com/v1alpha1\nkind: ManagedCluster\nmetadata:\n  name: cluster-1\nspec:\n  template: aws-eks-0-0-2\n  credential: aws-cred\n  config:\n    sshKeyName: foobar\n    region: ${AWS_REGION}\n    workersNumber: 1\n...\n</code></pre>"},{"location":"clustertemplates/aws/vpc-removal/","title":"Non-removed VPC","text":"<p>A bug was fixed in CAPA (Cluster API Provider AWS) for VPC removal: kubernetes-sigs/cluster-api-provider-aws#5192</p> <p>It is possible to deal with non-deleted VPCs the following ways:</p>"},{"location":"clustertemplates/aws/vpc-removal/#applying-ownership-information-on-vpcs","title":"Applying ownership information on VPCs","text":"<p>When VPCs have owner information, all AWS resources will be removed when 2A ESK cluster is deleted. So, after provisioning EKS cluster the operator can go and set tags (i.e. <code>tag:Owner</code>) and it will be sufficient for CAPA to manage them.</p>"},{"location":"clustertemplates/aws/vpc-removal/#guardduty-vpce","title":"GuardDuty VPCE","text":"<p>Another way to prevent an issue with non-deleted VPCs is to disable GuardDuty. GuardDuty creates an extra VPCE (VPC Endpoint) not managed by CAPA and when CAPA starts EKS cluster removal, this VPCE is not removed.</p>"},{"location":"clustertemplates/aws/vpc-removal/#manual-removal-of-vpcs","title":"Manual removal of VPCs","text":"<p>When it is impossible to turn off GuardDuty or applying ownership tags is not permitted, it is needed to remove VPCs manually.</p> <p>The sign of \u201cstuck\u201d VPC looks like a hidden \u201cDelete\u201d button. </p> <p>Opening \u201cNetwork Interfaces\u201d and attempting to detach an interface shows disable \u201cDetach\u201d button: </p> <p>It is required to get to VPC endpoints screen and remove the end-point:  </p> <p></p> <p>Wait until VPCE is completely removed, all network interfaces disappear. </p> <p>Now VPC can be finally removed: </p>"},{"location":"clustertemplates/azure/hosted-control-plane/","title":"Azure Hosted control plane (k0smotron) deployment","text":""},{"location":"clustertemplates/azure/hosted-control-plane/#prerequisites","title":"Prerequisites","text":"<ul> <li>Management Kubernetes cluster (v1.28+) deployed on Azure with HMC installed     on it</li> <li>Default storage class configured on the management cluster</li> </ul> <p>Keep in mind that all control plane components for all managed clusters will reside in the management cluster.</p>"},{"location":"clustertemplates/azure/hosted-control-plane/#pre-existing-resources","title":"Pre-existing resources","text":"<p>Certain resources will not be created automatically in a hosted control plane scenario thus they should be created in advance and provided in the <code>ManagedCluster</code> object. You can reuse these resources with management cluster as described below.</p> <p>If you deployed your Azure Kubernetes cluster using Cluster API Provider Azure (CAPZ) you can obtain all the necessary data with the commands below:</p> <p>Location</p> <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{.spec.location}}'\n</code></pre> <p>Subscription ID</p> <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{.spec.subscriptionID}}'\n</code></pre> <p>Resource group</p> <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{.spec.resourceGroup}}'\n</code></pre> <p>vnet name</p> <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{.spec.networkSpec.vnet.name}}'\n</code></pre> <p>Subnet name</p> <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{(index .spec.networkSpec.subnets 1).name}}'\n</code></pre> <p>Route table name</p> <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{(index .spec.networkSpec.subnets 1).routeTable.name}}'\n</code></pre> <p>Security group name</p> <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{(index .spec.networkSpec.subnets 1).securityGroup.name}}'\n</code></pre>"},{"location":"clustertemplates/azure/hosted-control-plane/#hmc-managedcluster-manifest","title":"HMC ManagedCluster manifest","text":"<p>With all the collected data your <code>ManagedCluster</code> manifest will look similar to this:</p> <pre><code>apiVersion: hmc.mirantis.com/v1alpha1\nkind: ManagedCluster\nmetadata:\n  name: azure-hosted-cp\nspec:\n  template: azure-hosted-cp-0-0-2\n  credential: azure-credential\n  config:\n    location: \"westus\"\n    subscriptionID: ceb131c7-a917-439f-8e19-cd59fe247e03\n    vmSize: Standard_A4_v2\n    resourceGroup: mgmt-cluster\n    network:\n      vnetName: mgmt-cluster-vnet\n      nodeSubnetName: mgmt-cluster-node-subnet\n      routeTableName: mgmt-cluster-node-routetable\n      securityGroupName: mgmt-cluster-node-nsg\n</code></pre> <p>To simplify creation of the ManagedCluster object you can use the template below:</p> <pre><code>apiVersion: hmc.mirantis.com/v1alpha1\nkind: ManagedCluster\nmetadata:\n  name: azure-hosted-cp\nspec:\n  template: azure-hosted-cp-0-0-2\n  credential: azure-credential\n  config:\n    location: \"{{.spec.location}}\"\n    subscriptionID: \"{{.spec.subscriptionID}}\"\n    vmSize: Standard_A4_v2\n    resourceGroup: \"{{.spec.resourceGroup}}\"\n    network:\n      vnetName: \"{{.spec.networkSpec.vnet.name}}\"\n      nodeSubnetName: \"{{(index .spec.networkSpec.subnets 1).name}}\"\n      routeTableName: \"{{(index .spec.networkSpec.subnets 1).routeTable.name}}\"\n      securityGroupName: \"{{(index .spec.networkSpec.subnets 1).securityGroup.name}}\"\n</code></pre> <p>Then you can render it using the command:</p> <pre><code>kubectl get azurecluster &lt;management-cluster-name&gt; -o go-template=\"$(cat template.yaml)\"\n</code></pre>"},{"location":"clustertemplates/azure/hosted-control-plane/#cluster-creation","title":"Cluster creation","text":"<p>After applying <code>ManagedCluster</code> object you require to manually set the status of the <code>AzureCluster</code> object due to current limitations (see k0sproject/k0smotron#668).</p> <p>To do so you need to execute the following command:</p> <pre><code>kubectl patch azurecluster &lt;cluster-name&gt; --type=merge --subresource status --patch 'status: {ready: true}'\n</code></pre>"},{"location":"clustertemplates/azure/hosted-control-plane/#important-notes-on-the-cluster-deletion","title":"Important notes on the cluster deletion","text":"<p>Because of the aforementioned limitation you also need to make manual steps in order to properly delete cluster.</p> <p>Before removing the cluster make sure to place custom finalizer onto <code>AzureCluster</code> object. This is needed to prevent it from being deleted instantly which will cause cluster deletion to stuck indefinitely.</p> <p>To place finalizer you can execute the following command:</p> <pre><code>kubectl patch azurecluster &lt;cluster-name&gt; --type=merge --patch 'metadata: {finalizers: [manual]}'\n</code></pre> <p>When finalizer is placed you can remove the <code>ManagedCluster</code> as usual. Check that all <code>AzureMachines</code> objects are deleted successfully and remove finalizer you've placed to finish cluster deletion.</p> <p>In case if have orphaned <code>AzureMachines</code> left you have to delete finalizers on them manually after making sure that no VMs are present in Azure.</p> <p>Note</p> <p> Since Azure admission prohibits orphaned objects mutation you'll have to disable it by deleting it's <code>mutatingwebhookconfiguration</code></p>"},{"location":"clustertemplates/azure/template-parameters/","title":"Azure machine parameters","text":""},{"location":"clustertemplates/azure/template-parameters/#ssh","title":"SSH","text":"<p>SSH public key can be passed to <code>.spec.config.sshPublicKey</code> (in case of hosted CP) parameter or <code>.spec.config.controlPlane.sshPublicKey</code> and <code>.spec.config.worker.sshPublicKey</code> parameters (in case of standalone CP) of the <code>ManagedCluster</code> object.</p> <p>It should be encoded in base64 format.</p>"},{"location":"clustertemplates/azure/template-parameters/#vm-size","title":"VM size","text":"<p>Azure supports various VM sizes which can be retrieved with the following command:</p> <pre><code>az vm list-sizes --location \"&lt;location&gt;\" -o table\n</code></pre> <p>Then desired VM size could be passed to the:</p> <ul> <li><code>.spec.config.vmSize</code> - for hosted CP deployment.</li> <li><code>.spec.config.controlPlane.vmSize</code> - for control plane nodes in the standalone   deployment.</li> <li><code>.spec.config.worker.vmSize</code> - for worker nodes in the standalone deployment.</li> </ul> <p>Example: Standard_A4_v2</p>"},{"location":"clustertemplates/azure/template-parameters/#root-volume-size","title":"Root Volume size","text":"<p>Root volume size of the VM (in GB) can be changed through the following parameters:</p> <ul> <li><code>.spec.config.rootVolumeSize</code> - for hosted CP deployment.</li> <li><code>.spec.config.controlPlane.rootVolumeSize</code> - for control plane nodes in the   standalone deployment.</li> <li><code>.spec.config.worker.rootVolumeSize</code> - for worker nodes in the standalone   deployment.</li> </ul> <p>Default value: 30</p> <p>Please note that this value can't be less than size of the root volume which defined in your image.</p>"},{"location":"clustertemplates/azure/template-parameters/#vm-image","title":"VM Image","text":"<p>You can define the image which will be used for you machine using the following parameters:</p> <ul> <li><code>.spec.config.image</code> - for hosted CP deployment.</li> <li><code>.spec.config.controlPlane.image</code> - for control plane nodes in the standalone   deployment.</li> <li><code>.spec.config.worker.image</code> - for worker nodes in the standalone deployment.</li> </ul> <p>There are multiple self-excluding ways to define the image source (e.g. Azure Compute Gallery, Azure Marketplace, etc.).</p> <p>Detailed information regarding image can be found in CAPZ documentation</p> <p>By default, the latest official CAPZ Ubuntu based image is used.</p>"},{"location":"clustertemplates/vsphere/hosted-control-plane/","title":"Hosted control plane (k0smotron) deployment","text":""},{"location":"clustertemplates/vsphere/hosted-control-plane/#prerequisites","title":"Prerequisites","text":"<ul> <li>Management Kubernetes cluster (v1.28+) deployed on vSphere with HMC installed   on it</li> </ul> <p>Keep in mind that all control plane components for all managed clusters will reside in the management cluster.</p>"},{"location":"clustertemplates/vsphere/hosted-control-plane/#managedcluster-manifest","title":"ManagedCluster manifest","text":"<p>Hosted CP template has mostly identical parameters with standalone CP, you can check them in the template parameters section.</p> <p>Important Note on Control Plane Endpoint IP Address</p> <p> The vSphere provider requires the control plane endpoint IP to be specified before deploying the cluster. Ensure that this IP matches the IP assigned to the k0smotron load balancer (LB) service. Provide the control plane endpoint IP to the k0smotron service via an annotation accepted by your LB provider (e.g., the <code>kube-vip</code> annotation in the example below).</p> <pre><code>apiVersion: hmc.mirantis.com/v1alpha1\nkind: ManagedCluster\nmetadata:\n  name: cluster-1\nspec:\n  template: vsphere-hosted-cp-0-0-2\n  credential: vsphere-credential\n  config:\n    vsphere:\n      server: vcenter.example.com\n      thumbprint: \"00:00:00\"\n      datacenter: \"DC\"\n      datastore: \"/DC/datastore/DC\"\n      resourcePool: \"/DC/host/vCluster/Resources/ResPool\"\n      folder: \"/DC/vm/example\"\n    controlPlaneEndpointIP: \"172.16.0.10\"\n    ssh:\n      user: ubuntu\n      publicKey: |\n        ssh-rsa AAA...\n    rootVolumeSize: 50\n    cpus: 2\n    memory: 4096\n    vmTemplate: \"/DC/vm/template\"\n    network: \"/DC/network/Net\"\n    k0smotron:\n      service:\n        annotations:\n          kube-vip.io/loadbalancerIPs: \"172.16.0.10\"\n</code></pre>"},{"location":"clustertemplates/vsphere/template-parameters/","title":"vSphere cluster template parameters","text":""},{"location":"clustertemplates/vsphere/template-parameters/#managedcluster-parameters","title":"ManagedCluster parameters","text":"<p>To deploy managed cluster a number of parameters should be passed to the <code>ManagedCluster</code> object.</p>"},{"location":"clustertemplates/vsphere/template-parameters/#parameter-list","title":"Parameter list","text":"<p>The following is the list of vSphere specific parameters, which are required for successful cluster creation.</p> Parameter Example Description <code>.spec.config.vsphere.server</code> <code>vcenter.example.com</code> Address of the vSphere server <code>.spec.config.vsphere.thumbprint</code> <code>\"00:00:00\"</code> Certificate thumbprint <code>.spec.config.vsphere.datacenter</code> <code>DC</code> Datacenter name <code>.spec.config.vsphere.datastore</code> <code>/DC/datastore/DS</code> Datastore path <code>.spec.config.vsphere.resourcePool</code> <code>/DC/host/vCluster/Resources/ResPool</code> Resource pool path <code>.spec.config.vsphere.folder</code> <code>/DC/vm/example</code> vSphere folder path <p>To obtain vSphere certificate thumbprint you can use the following command:</p> <pre><code>curl -sw %{certs} https://vcenter.example.com | openssl x509 -sha256 -fingerprint -noout | awk -F '=' '{print $2}'\n</code></pre>"},{"location":"clustertemplates/vsphere/template-parameters/#example-of-managedcluster-cr","title":"Example of ManagedCluster CR","text":"<p>With all above parameters provided your <code>ManagedCluster</code> can look like this:</p> <pre><code>apiVersion: hmc.mirantis.com/v1alpha1\nkind: ManagedCluster\nmetadata:\n  name: cluster-1\nspec:\n  template: vsphere-standalone-cp-0-0-2\n  credential: vsphere-credential\n  config:\n    vsphere:\n      server: vcenter.example.com\n      thumbprint: \"00:00:00\"\n      datacenter: \"DC\"\n      datastore: \"/DC/datastore/DC\"\n      resourcePool: \"/DC/host/vCluster/Resources/ResPool\"\n      folder: \"/DC/vm/example\"\n    controlPlaneEndpointIP: \"172.16.0.10\"\n\n    controlPlane:\n      ssh:\n        user: ubuntu\n        publicKey: |\n          ssh-rsa AAA...\n      rootVolumeSize: 50\n      cpus: 2\n      memory: 4096\n      vmTemplate: \"/DC/vm/template\"\n      network: \"/DC/network/Net\"\n\n    worker:\n      ssh:\n        user: ubuntu\n        publicKey: |\n          ssh-rsa AAA...\n      rootVolumeSize: 50\n      cpus: 2\n      memory: 4096\n      vmTemplate: \"/DC/vm/template\"\n      network: \"/DC/network/Net\"\n</code></pre>"},{"location":"clustertemplates/vsphere/template-parameters/#ssh","title":"SSH","text":"<p>Currently SSH configuration on vSphere expects that user is already created during template creation. Because of that you must pass username along with SSH public key to configure SSH access.</p> <p>SSH public key can be passed to <code>.spec.config.ssh.publicKey</code> (in case of hosted CP) parameter or <code>.spec.config.controlPlane.ssh.publicKey</code> and <code>.spec.config.worker.ssh.publicKey</code> parameters (in case of standalone CP) of the <code>ManagedCluster</code> object.</p> <p>SSH public key must be passed literally as a string.</p> <p>Username can be passed to <code>.spec.config.controlPlane.ssh.user</code>, <code>.spec.config.worker.ssh.user</code> or <code>.spec.config.ssh.user</code> depending on you deployment model.</p>"},{"location":"clustertemplates/vsphere/template-parameters/#vm-resources","title":"VM resources","text":"<p>The following parameters are used to define VM resources:</p> Parameter Example Description <code>.rootVolumeSize</code> <code>50</code> Root volume size in GB (can't be less than one defined in the image) <code>.cpus</code> <code>2</code> Number of CPUs <code>.memory</code> <code>4096</code> Memory size in MB <p>The resource parameters are the same for hosted and standalone CP deployments, but they are positioned differently in the spec, which means that they're going to:</p> <ul> <li><code>.spec.config</code> in case of hosted CP deployment.</li> <li><code>.spec.config.controlPlane</code> in in case of standalone CP for control plane   nodes.</li> <li><code>.spec.config.worker</code> in in case of standalone CP for worker nodes.</li> </ul>"},{"location":"clustertemplates/vsphere/template-parameters/#vm-image-and-network","title":"VM Image and network","text":"<p>To provide image template path and network path the following parameters must be used:</p> Parameter Example Description <code>.vmTemplate</code> <code>/DC/vm/template</code> Image template path <code>.network</code> <code>/DC/network/Net</code> Network path <p>As with resource parameters the position of these parameters in the <code>ManagedCluster</code> depends on deployment type and these parameters are used in:</p> <ul> <li><code>.spec.config</code> in case of hosted CP deployment.</li> <li><code>.spec.config.controlPlane</code> in in case of standalone CP for control plane   nodes.</li> <li><code>.spec.config.worker</code> in in case of standalone CP for worker nodes.</li> </ul>"},{"location":"credential/main/","title":"Credential System","text":"<p>In order for infrastructure provider to work properly a correct credentials should be passed to it. The following describes how it is implemented in Project 2A.</p>"},{"location":"credential/main/#the-process","title":"The process","text":"<p>The following is the process of passing credentials to the system:</p> <ol> <li>Provider specific <code>ClusterIdentity</code> and <code>Secret</code> are created</li> <li><code>Credential</code> object is created referencing <code>ClusterIdentity</code> from step 1.</li> <li>The <code>Credential</code> object is then referenced in the <code>ManagedCluster</code>.</li> <li>Optionally, certain credentials MAY be propagated to the <code>ManagedCluster</code> after it is created.</li> </ol> <p>The following diagram illustrates the process:</p> <pre><code>flowchart TD\n  Step1[\"&lt;b&gt;Step 1&lt;/b&gt; (Lead Engineer):&lt;br/&gt;Create ClusterIdentity and Secret objects where ClusterIdentity references Secret\"]\n  Step1 --&gt; Step2[\"&lt;b&gt;Step 2&lt;/b&gt; (Any Engineer):&lt;br/&gt;Create Credential object referencing ClusterIdentity\"]\n  Step2 --&gt; Step3[\"&lt;b&gt;Step 3&lt;/b&gt; (Any Engineer):&lt;br/&gt;Create ManagedCluster referencing Credential object\"]\n  Step3 --&gt; Step4[\"&lt;b&gt;Step 4&lt;/b&gt; (Any Engineer):&lt;br/&gt;Apply ManagedCluster, wait for provisioning &amp; reconciliation, then propagate credentials to nodes if necessary\"]</code></pre> <p>By design steps 1 and 2 should be executed by the lead engineer who has access to the credentials. Thus credentials could be used by engineers without a need to have access to actual credentials or underlying resources, like <code>ClusterIdentity</code>.</p>"},{"location":"credential/main/#credential-object","title":"Credential object","text":"<p>The <code>Credential</code> object acts like a reference to the underlying credentials. It is namespace-scoped, which means that it must be in the same <code>Namespace</code> with the <code>ManagedCluster</code> it is referenced in. Actual credentials can be located in any namespace.</p>"},{"location":"credential/main/#example","title":"Example","text":"<pre><code>---\napiVersion: hmc.mirantis.com/v1alpha1\nkind: Credential\nmetadata:\n  name: azure-credential\n  namespace: dev\nspec:\n  description: \"Main Azure credentials\"\n  identityRef:\n    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\n    kind: AzureClusterIdentity\n    name: azure-cluster-identity\n    namespace: hmc-system\n</code></pre> <p>In the example above <code>Credential</code> object is referencing <code>AzureClusterIdentity</code> which was created in the <code>hmc-system</code> namespace.</p> <p>The <code>.spec.description</code> field can be used to provide arbitrary description of the object, so user could make a decision which credentials to use if several are present.</p>"},{"location":"credential/main/#cloud-provider-credentials-propagation","title":"Cloud provider credentials propagation","text":"<p>Some components in the managed cluster require cloud provider credentials to be passed for proper functioning. As an example Cloud Controller Manager (CCM) requires provider credentials to create load balancers and provide other functionality.</p> <p>This poses a challenge of credentials delivery. Currently <code>cloud-init</code> is used to pass all necessary credentials. This approach has several problems:</p> <ul> <li>Credentials stored unencrypted in the instance metadata.</li> <li>Rotation of the credentials is impossible without complete instance   redeployment.</li> <li>Possible leaks, since credentials are copied to several <code>Secret</code> objects   related to bootstrap data.</li> </ul> <p>To solve these problems in Project 2A we're using special controller which aggregates all necessary data from CAPI provider resources (like <code>ClusterIdentity</code>) and creates secrets directly on the managed cluster.</p> <p>This eliminates the need to pass anything credentials-related to <code>cloud-init</code> and makes it possible to rotate credentials automatically without the need for instance redeployment.</p> <p>Also this automation makes it possible to separate roles and responsibilities where only the lead engineer has access to credentials and other engineers can use them without seeing values and even any access to underlying infrastructure platform.</p> <p>The process is fully automated and credentials will be propagated automatically within the <code>ManagedCluster</code> reconciliation process, user only needs to provide the correct Credential object.</p>"},{"location":"credential/main/#provider-specific-notes","title":"Provider specific notes","text":"<p>Since this feature depends on the provider some notes and clarifications are needed for each provider.</p> <p>Note</p> <p>More detailed research notes can be found here.</p>"},{"location":"credential/main/#aws","title":"AWS","text":"<p>Since AWS uses roles, which are assigned to instances, no additional credentials will be created.</p> <p>AWS provider supports 3 types of <code>ClusterIdentity</code>, which one to use depends on your specific use case. More information regarding CAPA <code>ClusterIdentity</code> resources could be found in CRD Reference.</p>"},{"location":"credential/main/#azure","title":"Azure","text":"<p>Currently Cluster API provider Azure (CAPZ) creates <code>azure.json</code> Secrets in the same namespace with <code>Cluster</code> object. By design they should be referenced in the <code>cloud-init</code> YAML later during bootstrap process.</p> <p>In Project 2A these Secrets aren't used and will not be added to the <code>cloud-init</code>, but engineers can access them unrestricted.</p>"},{"location":"quick-start/2a-installation/","title":"2A Installation","text":""},{"location":"quick-start/2a-installation/#requirements","title":"Requirements","text":"<p>Project 2A requires a Kubernetes cluster. It can be of any type and will become the 2A management cluster.</p> <p>If you don't have a Kubernetes cluster yet, consider using k0s.</p> <p>The following instructions assume:</p> <ul> <li>Your <code>kubeconfig</code> points to the correct Kubernetes cluster.</li> <li>You have Helm installed.</li> <li>You have kubectl installed.</li> </ul>"},{"location":"quick-start/2a-installation/#helpful-tools","title":"Helpful Tools","text":"<p>It may be helpful to have the following tools installed:</p> <ul> <li>clusterctl</li> <li>Mirantis Lens</li> <li>k9s</li> </ul>"},{"location":"quick-start/2a-installation/#installation-via-helm","title":"Installation via Helm","text":"<pre><code>helm install hmc oci://ghcr.io/mirantis/hmc/charts/hmc --version 0.0.3 -n hmc-system --create-namespace\n</code></pre>"},{"location":"quick-start/2a-installation/#verification","title":"Verification","text":"<p>The installation takes a couple of minutes until 2A and its subcomponents are fully installed and configured.</p> <p>Verify the installation by checking all the pods in the <code>hmc-system</code> namespace with the following command:</p> <pre><code>kubectl get pods -n hmc-system\n</code></pre> <p>The output is similar to:</p> <pre><code>NAME                                                           READY   STATUS\nazureserviceoperator-controller-manager-86d566cdbc-rqkt9       1/1     Running\ncapa-controller-manager-7cd699df45-28hth                       1/1     Running\ncapi-controller-manager-6bc5fc5f88-hd8pv                       1/1     Running\ncapv-controller-manager-bb5ff9bd5-7dsr9                        1/1     Running\ncapz-controller-manager-5dd988768-qjdbl                        1/1     Running\nhelm-controller-76f675f6b7-4d47l                               1/1     Running\nhmc-cert-manager-7c8bd964b4-nhxnq                              1/1     Running\nhmc-cert-manager-cainjector-56476c46f9-xvqhh                   1/1     Running\nhmc-cert-manager-webhook-69d7fccf68-s46w8                      1/1     Running\nhmc-cluster-api-operator-79459d8575-2s9jc                      1/1     Running\nhmc-controller-manager-64869d9f9d-zktgw                        1/1     Running\nk0smotron-controller-manager-bootstrap-6c5f6c7884-d2fqs        2/2     Running\nk0smotron-controller-manager-control-plane-857b8bffd4-zxkx2    2/2     Running\nk0smotron-controller-manager-infrastructure-7f77f55675-tv8vb   2/2     Running\nsource-controller-5f648d6f5d-7mhz5                             1/1     Running\n</code></pre> <p>If you have fewer pods, give 2A more time to reconcile all the pods.</p> <p>As a second verification, check that the example <code>ClusterTemplate</code> objects have been installed and are valid:</p> <pre><code>kubectl get clustertemplate -n hmc-system\n</code></pre> <p>The output is similar to:</p> <pre><code>NAME                                VALID\naws-eks-0-0-1                       true\naws-hosted-cp-0-0-2                 true\naws-standalone-cp-0-0-1             true\naws-standalone-cp-0-1-0             true\nazure-hosted-cp-0-0-3               true\nazure-standalone-cp-0-0-1           true\nazure-standalone-cp-0-1-0           true\nremote-single-standalone-cp-0-1-4   true\nremote-single-standalone-cp-0-1-5   true\nremote-single-standalone-cp-0-1-6   true\nremote-single-standalone-cp-0-1-7   true\nremote-single-standalone-cp-0-1-8   true\nvsphere-hosted-cp-0-0-2             true\nvsphere-standalone-cp-0-0-2         true\n</code></pre>"},{"location":"quick-start/2a-installation/#next-step","title":"Next Step","text":"<p>Now you can configure your Infrastructure Provider of choice and create your first Managed Cluster.</p> <p>Jump to any of the following Infrastructure Providers for specific instructions:</p> <ul> <li>AWS Quick Start</li> <li>Azure Quick Start</li> <li>vSphere Quick Start</li> </ul>"},{"location":"quick-start/aws/","title":"AWS Quick Start","text":"<p>Much of the following includes the process of setting up credentials for AWS. To better understand how Project 2A uses credentials, read the Credential system.</p>"},{"location":"quick-start/aws/#prerequisites","title":"Prerequisites","text":""},{"location":"quick-start/aws/#2a-management-cluster","title":"2A Management Cluster","text":"<p>You need a Kubernetes cluster with 2A installed.</p>"},{"location":"quick-start/aws/#software-prerequisites","title":"Software prerequisites","text":"<p>The AWS <code>clusterawsadm</code> tool is required to bootstrap an AWS Account. Install it by following the AWS clusterawsadm installation instructions.</p>"},{"location":"quick-start/aws/#eks-deployment","title":"EKS Deployment","text":"<ul> <li>Additional EKS steps and verifications are described in EKS clusters.</li> </ul>"},{"location":"quick-start/aws/#configure-aws-iam","title":"Configure AWS IAM","text":"<p>Before launching a cluster on AWS, you need to set up your AWS infrastructure with the necessary IAM policies and service account.</p> <p>Note</p> <p> Skip steps below if you've already configured IAM policy for your AWS account</p> <ol> <li> <p>To use <code>clusterawsadm</code>, you must have an administrative user in an AWS    account. Once you have that administrator user, set your environment    variables:</p> <pre><code>export AWS_REGION=&lt;aws-region&gt;\nexport AWS_ACCESS_KEY_ID=&lt;admin-user-access-key&gt;\nexport AWS_SECRET_ACCESS_KEY=&lt;admin-user-secret-access-key&gt;\nexport AWS_SESSION_TOKEN=&lt;session-token&gt; # Optional. If you are using Multi-Factor Auth.\n</code></pre> </li> <li> <p>After these are set, run this command to create the IAM CloudFormation stack:</p> <pre><code>clusterawsadm bootstrap iam create-cloudformation-stack\n</code></pre> </li> </ol>"},{"location":"quick-start/aws/#step-1-create-aws-iam-user","title":"Step 1: Create AWS IAM User","text":"<ol> <li> <p>Create an AWS IAM user assigned the following roles:</p> <ul> <li><code>control-plane.cluster-api-provider-aws.sigs.k8s.io</code></li> <li><code>controllers.cluster-api-provider-aws.sigs.k8s.io</code></li> <li><code>nodes.cluster-api-provider-aws.sigs.k8s.io</code></li> </ul> </li> <li> <p>Create Access Keys for the IAM user.</p> <p>In the AWS IAM Console, create the Access Keys for the IAM user and download them.</p> <p>You should have an <code>AccessKeyID</code> and a <code>SecretAccessKey</code> that look like the following:</p> Access key ID Secret access key AKIAQF+EXAMPLE EdJfFar6+example </li> </ol>"},{"location":"quick-start/aws/#step-2-create-the-iam-credentials-secret-on-2a-management-cluster","title":"Step 2: Create the IAM Credentials Secret on 2A Management Cluster","text":"<p>Save the <code>Secret</code> YAML to a file named <code>aws-cluster-identity-secret.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: aws-cluster-identity-secret\n  namespace: hmc-system\ntype: Opaque\nstringData:\n  AccessKeyID: AKIAQF+EXAMPLE\n  SecretAccessKey: EdJfFar6+example\n</code></pre> <p>Apply the YAML to your cluster using the following command:</p> <pre><code>kubectl apply -f aws-cluster-identity-secret.yaml\n</code></pre> <p>Warning</p> <p> The secret must be created in the same <code>Namespace</code> where the CAPA provider is running. In case of Project 2A it's currently <code>hmc-system</code>. Placing secret in any other <code>Namespace</code> will result in the controller not able to read it.</p>"},{"location":"quick-start/aws/#step-3-create-awsclusterstaticidentity-object","title":"Step 3: Create AWSClusterStaticIdentity Object","text":"<p>Save the <code>AWSClusterStaticIdentity</code> YAML into a file named <code>aws-cluster-identity.yaml</code>:</p> <p>Note</p> <p> <code>.spec.secretRef</code> must match <code>.metadata.name</code> of the secret that was created in the previous step.</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta2\nkind: AWSClusterStaticIdentity\nmetadata:\n  name: aws-cluster-identity\nspec:\n  secretRef: aws-cluster-identity-secret\n  allowedNamespaces:\n    selector:\n      matchLabels: {}\n</code></pre> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f aws-cluster-identity.yaml\n</code></pre>"},{"location":"quick-start/aws/#step-4-create-the-2a-credential-object","title":"Step 4: Create the 2A Credential Object","text":"<p>Create a YAML with the specification of your credential and save it as <code>aws-cluster-identity-cred.yaml</code>.</p> <p>Note</p> <p> <code>.spec.identityRef.kind</code> must be <code>AWSClusterStaticIdentity</code> and the <code>.spec.identityRef.name</code> must match the <code>.metadata.name</code> of the <code>AWSClusterStaticIdentity</code> object.</p> <pre><code>apiVersion: hmc.mirantis.com/v1alpha1\nkind: Credential\nmetadata:\n  name: aws-cluster-identity-cred\n  namespace: hmc-system\nspec:\n  description: \"Credential Example\"\n  identityRef:\n    apiVersion: infrastructure.cluster.x-k8s.io/v1beta2\n    kind: AWSClusterStaticIdentity\n    name: aws-cluster-identity\n</code></pre> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f aws-cluster-identity-cred.yaml\n</code></pre>"},{"location":"quick-start/aws/#step-5-create-your-first-managed-cluster","title":"Step 5: Create Your First Managed Cluster","text":"<p>Create a YAML with the specification of your Managed Cluster and save it as <code>my-aws-managedcluster1.yaml</code>.</p> <p>Here is an example of a <code>ManagedCluster</code> YAML file:</p> <pre><code>apiVersion: hmc.mirantis.com/v1alpha1\nkind: ManagedCluster\nmetadata:\n  name: my-aws-managedcluster1\n  namespace: hmc-system\nspec:\n  template: aws-standalone-cp-0-0-2 # The name of the template you want to use from above\n  credential: aws-cluster-identity-cred\n  config:\n    region: us-west-2\n    controlPlane:\n      instanceType: t3.small\n    worker:\n      instanceType: t3.small\n</code></pre> <p>Apply the YAML to your management cluster:</p> <pre><code>kubectl apply -f my-aws-managedcluster1.yaml\n</code></pre> <p>There will be a delay as the cluster finishes provisioning. Follow the provisioning process with the following command:</p> <pre><code>kubectl -n hmc-system get managedcluster.hmc.mirantis.com my-aws-managedcluster1 --watch\n</code></pre> <p>After the cluster is <code>Ready</code>, you can access it via the kubeconfig, like this:</p> <pre><code>kubectl -n hmc-system get secret my-aws-managedcluster1-kubeconfig -o jsonpath='{.data.value}' | base64 -d &gt; my-aws-managedcluster1-kubeconfig.kubeconfig\n</code></pre> <pre><code>KUBECONFIG=\"my-aws-managedcluster1-kubeconfig.kubeconfig\" kubectl get pods -A\n</code></pre>"},{"location":"quick-start/azure/","title":"Azure Quick Start","text":"<p>Much of the following includes the process of setting up credentials for Azure. To better understand how Project 2A uses credentials, read the Credential System.</p>"},{"location":"quick-start/azure/#prerequisites","title":"Prerequisites","text":""},{"location":"quick-start/azure/#2a-management-cluster","title":"2A Management Cluster","text":"<p>You need a Kubernetes cluster with 2A installed.</p>"},{"location":"quick-start/azure/#software-prerequisites","title":"Software prerequisites","text":"<p>Before deploying Kubernetes clusters on Azure using Project 2A, ensure you have:</p> <p>The Azure CLI (<code>az</code>) is required to interact with Azure resources. Install it by following the Azure CLI installation instructions.</p> <p>Run the <code>az login</code> command to authenticate your session with Azure.</p>"},{"location":"quick-start/azure/#register-resource-providers","title":"Register resource providers","text":"<p>If you're using a new subscription that has never been used to deploy 2A or CAPI clusters, ensure the following resource providers are registered:</p> <ul> <li><code>Microsoft.Compute</code></li> <li><code>Microsoft.Network</code></li> <li><code>Microsoft.ContainerService</code></li> <li><code>Microsoft.ManagedIdentity</code></li> <li><code>Microsoft.Authorization</code></li> </ul> <p>To register these providers, run the following commands in the Azure CLI:</p> <pre><code>az provider register --namespace Microsoft.Compute\naz provider register --namespace Microsoft.Network\naz provider register --namespace Microsoft.ContainerService\naz provider register --namespace Microsoft.ManagedIdentity\naz provider register --namespace Microsoft.Authorization\n</code></pre> <p>You can follow the official documentation guide to register the providers.</p> <p>Before creating a cluster on Azure, set up credentials. This involves creating an <code>AzureClusterIdentity</code> and a Service Principal (SP) to let CAPZ (Cluster API Azure) communicate with Azure.</p>"},{"location":"quick-start/azure/#step-1-find-your-subscription-id","title":"Step 1: Find Your Subscription ID","text":"<p>List all your Azure subscriptions:</p> <pre><code>az account list -o table\n</code></pre> <p>Look for the Subscription ID of the account you want to use.</p> <p>Example output:</p> <pre><code>Name                     SubscriptionId                        TenantId\n-----------------------  -------------------------------------  --------------------------------\nMy Azure Subscription    12345678-1234-5678-1234-567812345678  87654321-1234-5678-1234-12345678\n</code></pre> <p>Copy your chosen Subscription ID for the next step.</p>"},{"location":"quick-start/azure/#step-2-create-a-service-principal-sp","title":"Step 2: Create a Service Principal (SP)","text":"<p>The Service Principal is like a password-protected user that CAPZ will use to manage resources on Azure.</p> <p>In your terminal, run the following command. Replace <code>&lt;subscription-id&gt;</code> with the ID you copied earlier:</p> <pre><code>az ad sp create-for-rbac --role contributor --scopes=\"/subscriptions/&lt;subscription-id&gt;\"\n</code></pre> <p>You will see output like this:</p> <pre><code>{\n \"appId\": \"12345678-7848-4ce6-9be9-a4b3eecca0ff\",\n \"displayName\": \"azure-cli-2024-10-24-17-36-47\",\n \"password\": \"12~34~I5zKrL5Kem2aXsXUw6tIig0M~3~1234567\",\n \"tenant\": \"12345678-959b-481f-b094-eb043a87570a\"\n}\n</code></pre> <p>Note</p> <p> Make sure to treat these strings like passwords. Do not share them or check them into a repository.</p>"},{"location":"quick-start/azure/#step-3-create-a-secret-object-with-the-password","title":"Step 3: Create a Secret Object with the password","text":"<p>The Secret stores the <code>clientSecret</code> (password) from the Service Principal.</p> <p>Save the Secret YAML into a file named <code>azure-cluster-identity-secret.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: azure-cluster-identity-secret\n  namespace: hmc-system\nstringData:\n  clientSecret: &lt;password&gt; # Password retrieved from the Service Principal\ntype: Opaque\n</code></pre> <p>Apply the YAML to your cluster using the following command:</p> <pre><code>kubectl apply -f azure-cluster-identity-secret.yaml\n</code></pre>"},{"location":"quick-start/azure/#step-4-create-the-azureclusteridentity-object","title":"Step 4: Create the AzureClusterIdentity Object","text":"<p>This object defines the credentials CAPZ will use to manage Azure resources. It references the Secret you just created above.</p> <p>Warning</p> <p> Make sure that <code>.spec.clientSecret.name</code> matches the name of the Secret you created in the previous step.</p> <p>Save the following YAML into a file named <code>azure-cluster-identity.yaml</code>:</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\nkind: AzureClusterIdentity\nmetadata:\n  labels:\n    clusterctl.cluster.x-k8s.io/move-hierarchy: \"true\"\n  name: azure-cluster-identity\n  namespace: hmc-system\nspec:\n  allowedNamespaces: {}\n  clientID: &lt;appId&gt; # The App ID retrieved from the Service Principal above in Step 2\n  clientSecret:\n    name: azure-cluster-identity-secret\n    namespace: hmc-system\n  tenantID: &lt;tenant&gt; # The Tenant ID retrieved from the Service Principal above in Step 2\n  type: ServicePrincipal\n</code></pre> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f azure-cluster-identity.yaml\n</code></pre>"},{"location":"quick-start/azure/#step-5-create-the-2a-credential-object","title":"Step 5: Create the 2A Credential Object","text":"<p>Create a YAML with the specification of our credential and save it as <code>azure-cluster-identity-cred.yaml</code>.</p> <p>Warning</p> <p> <code>.spec.kind</code> must be <code>AzureClusterIdentity</code> <code>.spec.name</code> must match <code>.metadata.name</code> of the <code>AzureClusterIdentity</code> object created in the previous step.</p> <pre><code>apiVersion: hmc.mirantis.com/v1alpha1\nkind: Credential\nmetadata:\n  name: azure-cluster-identity-cred\n  namespace: hmc-system\nspec:\n  identityRef:\n    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\n    kind: AzureClusterIdentity\n    name: azure-cluster-identity\n    namespace: hmc-system\n</code></pre> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f azure-cluster-identity-cred.yaml\n</code></pre> <p>This creates the <code>Credential</code> object that will be used in the next step.</p>"},{"location":"quick-start/azure/#step-6-create-your-first-managedcluster","title":"Step 6: Create your first ManagedCluster","text":"<p>Create a YAML with the specification of your managed Cluster and save it as <code>my-azure-managedcluster1.yaml</code>.</p> <p>Here is an example of a <code>ManagedCluster</code> YAML file:</p> <pre><code>apiVersion: hmc.mirantis.com/v1alpha1\nkind: ManagedCluster\nmetadata:\n  name: my-azure-managedcluster1\n  namespace: hmc-system\nspec:\n  template: azure-standalone-cp-0-0-2\n  credential: azure-cluster-identity-cred\n  config:\n    location: \"westus\" # Select your desired Azure Location (find it via `az account list-locations -o table`)\n    subscriptionID: &lt;subscription-id&gt; # Enter the Subscription ID used earlier\n    controlPlane:\n      vmSize: Standard_A4_v2\n    worker:\n      vmSize: Standard_A4_v2\n</code></pre> <p>Apply the YAML to your management cluster:</p> <pre><code>kubectl apply -f my-azure-managedcluster1.yaml\n</code></pre> <p>There will be a delay as the cluster finishes provisioning. Follow the provisioning process with the following command:</p> <pre><code>kubectl -n hmc-system get managedcluster.hmc.mirantis.com my-azure-managedcluster1 --watch\n</code></pre> <p>After the cluster is <code>Ready</code>, you can access it via the kubeconfig, like this:</p> <pre><code>kubectl -n hmc-system get secret my-azure-managedcluster1-kubeconfig -o jsonpath='{.data.value}' | base64 -d &gt; my-azure-managedcluster1-kubeconfig.kubeconfig\n</code></pre> <pre><code>KUBECONFIG=\"my-azure-managedcluster1-kubeconfig.kubeconfig\" kubectl get pods -A\n</code></pre>"},{"location":"quick-start/vsphere/","title":"vSphere Quick Start","text":"<p>Much of the following includes the process of setting up credentials for vSphere. To better understand how Project 2A uses credentials, read the Credential System.</p>"},{"location":"quick-start/vsphere/#prerequisites","title":"Prerequisites","text":""},{"location":"quick-start/vsphere/#2a-management-cluster","title":"2A Management Cluster","text":"<p>You need a Kubernetes cluster with 2A installed.</p>"},{"location":"quick-start/vsphere/#software-vmware-specific-prerequisites","title":"Software &amp; VMware-specific prerequisites","text":"<ol> <li><code>kubectl</code> CLI installed locally.</li> <li>vSphere instance version <code>6.7.0</code> or higher.</li> <li>vSphere account with appropriate privileges.</li> <li>Image template.</li> <li>vSphere network with DHCP enabled.</li> </ol>"},{"location":"quick-start/vsphere/#vsphere-privileges","title":"vSphere privileges","text":"<p>To function properly, the user assigned to the vSphere Provider should be able to manipulate vSphere resources. The following is the general overview of the required privileges:</p> <ul> <li><code>Virtual machine</code> - full permissions are required</li> <li><code>Network</code> - <code>Assign network</code> is sufficient</li> <li><code>Datastore</code> - it should be possible for user to manipulate virtual machine   files and metadata</li> </ul> <p>In addition to that, specific CSI driver permissions are required. See the official doc for more information on CSI-specific permissions.</p>"},{"location":"quick-start/vsphere/#image-template","title":"Image template","text":"<p>You can use pre-built image templates from the CAPV project or build your own.</p> <p>When building your own image, make sure that VMware tools and cloud-init are installed and properly configured.</p> <p>You can follow the official open-vm-tools guide on how to correctly install VMware tools.</p> <p>When setting up cloud-init, you can refer to the official docs and specifically the VMware datasource docs for extended information regarding cloud-init on vSphere.</p>"},{"location":"quick-start/vsphere/#vsphere-network","title":"vSphere network","text":"<p>When creating a network, make sure that it has DHCP service.</p> <p>Also, ensure that part of your network is out of the DHCP range (e.g., network <code>172.16.0.0/24</code> should have DHCP range <code>172.16.0.100-172.16.0.254</code> only). This is needed to ensure that LB services will not create any IP conflicts in the network.</p>"},{"location":"quick-start/vsphere/#step-1-create-a-secret-object-with-the-username-and-password","title":"Step 1: Create a Secret Object with the username and password","text":"<p>The Secret stores the username and password for your vSphere instance.</p> <p>Save the Secret YAML into a file named <code>vsphere-cluster-identity-secret.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: vsphere-cluster-identity-secret\n  namespace: hmc-system\nstringData:\n  username: &lt;user&gt;\n  password: &lt;password&gt;\ntype: Opaque\n</code></pre> <p>Apply the YAML to your cluster using the following command:</p> <pre><code>kubectl apply -f vsphere-cluster-identity-secret.yaml\n</code></pre>"},{"location":"quick-start/vsphere/#step-2-create-the-vsphereclusteridentity-object","title":"Step 2: Create the VSphereClusterIdentity Object","text":"<p>This object defines the credentials CAPV will use to manage vSphere resources.</p> <p>Save the VSphereClusterIdentity YAML into a file named <code>vsphere-cluster-identity.yaml</code>:</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\nkind: VSphereClusterIdentity\nmetadata:\n  name: vsphere-cluster-identity\nspec:\n  secretName: vsphere-cluster-identity-secret\n  allowedNamespaces:\n    selector:\n      matchLabels: {}\n</code></pre> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f vsphere-cluster-identity.yaml\n</code></pre>"},{"location":"quick-start/vsphere/#step-3-create-the-2a-credential-object","title":"Step 3: Create the 2A Credential Object","text":"<p>Create a YAML with the specification of our credential and save it as <code>vsphere-cluster-identity-cred.yaml</code></p> <pre><code>apiVersion: hmc.mirantis.com/v1alpha1\nkind: Credential\nmetadata:\n  name: vsphere-cluster-identity-cred\n  namespace: hmc-system\nspec:\n  identityRef:\n    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\n    kind: VSphereClusterIdentity\n    name: vsphere-cluster-identity\n</code></pre> <p>Warning</p> <p> <code>.spec.identityRef.kind</code> must be <code>VSphereClusterIdentity</code> and the <code>.spec.identityRef.name</code> must match the <code>.metadata.name</code> of the <code>VSphereClusterIdentity</code> object above.</p> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f vsphere-cluster-identity-cred.yaml\n</code></pre>"},{"location":"quick-start/vsphere/#step-4-create-your-first-managed-cluster","title":"Step 4: Create your first Managed Cluster","text":"<p>Create a YAML with the specification of your Managed Cluster and save it as <code>my-vsphere-managedcluster1.yaml</code>.</p> <p>Here is an example of a <code>ManagedCluster</code> YAML file:</p> <pre><code>apiVersion: hmc.mirantis.com/v1alpha1\nkind: ManagedCluster\nmetadata:\n  name: my-vsphere-managedcluster1\n  namespace: hmc-system\nspec:\n  template: &lt;template-name&gt; # The name of the template you want to use from above\n  credential: vsphere-cluster-identity-cred\n  config:\n    vsphere:\n      server: &lt;VSPHERE_SERVER&gt;\n      thumbprint: &lt;VSPHERE_THUMBPRINT&gt;\n      datacenter: &lt;VSPHERE_DATACENTER&gt;\n      datastore: &lt;VSPHERE_DATASTORE&gt;\n      resourcePool: &lt;VSPHERE_RESOURCEPOOL&gt;\n      folder: &lt;VSPHERE_FOLDER&gt;\n    controlPlaneEndpointIP: &lt;VSPHERE_CONTROL_PLANE_ENDPOINT&gt;\n    controlPlane:\n      ssh:\n        user: ubuntu\n        publicKey: &lt;VSPHERE_SSH_KEY&gt;\n      vmTemplate: &lt;VSPHERE_VM_TEMPLATE&gt;\n      network: &lt;VSPHERE_NETWORK&gt;\n    worker:\n      ssh:\n        user: ubuntu\n        publicKey: &lt;VSPHERE_SSH_KEY&gt;\n      vmTemplate: &lt;VSPHERE_VM_TEMPLATE&gt;\n      network: &lt;VSPHERE_NETWORK&gt;\n</code></pre> <p>Note</p> <p> For more information about the config options, see the vSphere Template Parameters.</p> <p>Apply the YAML to your management cluster:</p> <pre><code>kubectl apply -f my-vsphere-managedcluster1.yaml\n</code></pre> <p>There will be a delay as the cluster finishes provisioning. Follow the provisioning process with the following command:</p> <pre><code>kubectl -n hmc-system get managedcluster.hmc.mirantis.com my-vsphere-managedcluster1 --watch\n</code></pre> <p>After the cluster is <code>Ready</code>, you can access it via the kubeconfig, like this:</p> <pre><code>kubectl -n hmc-system get secret my-vsphere-managedcluster1-kubeconfig -o jsonpath='{.data.value}' | base64 -d &gt; my-vsphere-managedcluster1-kubeconfig.kubeconfig\n</code></pre> <pre><code>KUBECONFIG=\"my-vsphere-managedcluster1-kubeconfig.kubeconfig\" kubectl get pods -A\n</code></pre>"},{"location":"rbac/roles/","title":"2A Role-Based Access Control","text":"<p>2A leverages the Kubernetes RBAC system and provides a set of standard <code>ClusterRoles</code> with associated permissions. All <code>ClusterRoles</code> are created as part of the HMC helm chart. 2A roles are based on labels and aggregated permissions, meaning they automatically collect rules from other <code>ClusterRoles</code> with specific labels.</p> <p>The following table outlines the roles available in 2A, along with their respective read/write or read-only permissions:</p> Roles Global Admin Global Viewer Namespace Admin Namespace Editor Namespace Viewer Scope Global Global Namespace Namespace Namespace 2A management r/w r/o - - - Namespaces management r/w r/o - - - Provider Templates r/w r/o - - - Global Template Management r/w r/o - - - Multi Cluster Service Management r/w r/o - - - Template Chain Management r/w r/o r/w r/o r/o Cluster and Service Templates r/w r/o r/w r/o r/o Credentials r/w r/o r/w r/o r/o Flux Helm objects r/w r/o r/w r/o r/o Managed Clusters r/w r/o r/w r/w r/o"},{"location":"rbac/roles/#roles-definition","title":"Roles definition","text":"<p>This section provides an overview of all <code>ClusterRoles</code> available in 2A.</p> <p>Note</p> <p> The names of the <code>ClusterRoles</code> may have different prefix depending on the name of the HMC Helm chart. The <code>ClusterRoles</code> definitions below use the <code>hmc</code> prefix, which is the default name of the HMC Helm chart.</p>"},{"location":"rbac/roles/#global-admin","title":"Global Admin","text":"<p>The <code>Global Admin</code> role provides full administrative access across all the 2A system.</p> <p>Name: <code>hmc-global-admin-role</code></p> <p>Aggregation Rule: Includes all <code>ClusterRoles</code> with the labels:</p> <ul> <li><code>hmc.mirantis.com/aggregate-to-global-admin: true</code></li> <li><code>hmc.mirantis.com/aggregate-to-namespace-admin: true</code></li> <li><code>hmc.mirantis.com/aggregate-to-namespace-editor: true</code></li> </ul> <p>Permissions:</p> <ol> <li>Full access to 2A API</li> <li>Full access to Flux Helm repositories and Helm charts</li> <li>Full access to Cluster API identities</li> <li>Full access to namespaces and secrets</li> </ol> <p>Use case</p> <p>A user with the <code>Global Admin</code> role is authorized to perform the following actions:</p> <ol> <li>Manage the 2A configuration</li> <li>Manage namespaces in the management cluster</li> <li>Manage <code>Provider Templates</code>: add new templates or remove unneeded ones</li> <li>Manage <code>Cluster</code> and <code>Service Templates</code> in any namespace, including adding and removing templates</li> <li>Manage Flux <code>HelmRepositories</code> and <code>HelmCharts</code> in any namespace</li> <li>Manage access rules for <code>Cluster</code> and <code>Service Templates</code>, including distributing templates across namespaces using    <code>Template Chains</code></li> <li>Manage upgrade sequences for <code>Cluster</code> and <code>Service Templates</code></li> <li>Manage and deploy Services across multiple clusters in any namespace by modifying <code>MultiClusterService</code> resources</li> <li>Manage <code>ManagedClusters</code> in any namespace</li> <li>Manage <code>Credentials</code> and <code>secrets</code> in any namespace</li> <li>Upgrade 2A</li> <li>Uninstall 2A</li> </ol>"},{"location":"rbac/roles/#global-viewer","title":"Global Viewer","text":"<p>The <code>Global Viewer</code> role grants read-only access across the 2A system. It does not permit any modifications, including the creation of clusters.</p> <p>Name: <code>hmc-global-viewer-role</code></p> <p>Aggregation Rule: Includes all <code>ClusterRoles</code> with the labels:</p> <ul> <li><code>hmc.mirantis.com/aggregate-to-global-viewer: true</code></li> <li><code>hmc.mirantis.com/aggregate-to-namespace-viewer: true</code></li> </ul> <p>Permissions:</p> <ol> <li>Read access to 2A API</li> <li>Read access to Flux Helm repositories and Helm charts</li> <li>Read access to Cluster API identities</li> <li>Read access to namespaces and secrets</li> </ol> <p>Use case</p> <p>A user with the <code>Global Viewer</code> role is authorized to perform the following actions:</p> <ol> <li>View the 2A configuration</li> <li>List namespaces available in the management cluster</li> <li>List and get the detailed information about available <code>Provider Templates</code></li> <li>List available <code>Cluster</code> and <code>Service Templates</code> in any namespace</li> <li>List and view detailed information about Flux <code>HelmRepositories</code> and <code>HelmCharts</code> in any namespace</li> <li>View access rules for <code>Cluster</code> and <code>Service Templates</code>, including <code>Template Chains</code> in any namespace</li> <li>View full details about the created <code>MultiClusterService</code> objects</li> <li>List and view detailed information about <code>ManagedClusters</code> in any namespace</li> <li>List and view detailed information about created <code>Credentials</code> and <code>secrets</code> in any namespace</li> </ol>"},{"location":"rbac/roles/#namespace-admin","title":"Namespace Admin","text":"<p>The <code>Namespace Admin</code> role provides full administrative access within namespace.</p> <p>Name: <code>hmc-namespace-admin-role</code></p> <p>Aggregation Rule: Includes all <code>ClusterRoles</code> with the labels:</p> <ul> <li><code>hmc.mirantis.com/aggregate-to-namespace-admin: true</code></li> <li><code>hmc.mirantis.com/aggregate-to-namespace-editor: true</code></li> </ul> <p>Permissions:</p> <ol> <li>Full access to <code>ManagedClusters</code>, <code>Credentials</code>, <code>Cluster</code> and <code>Service Templates</code> in the namespace</li> <li>Full access to <code>Template Chains</code> in the namespace</li> <li>Full access to Flux <code>HelmRepositories</code> and <code>HelmCharts</code> in the namespace</li> </ol> <p>Use case</p> <p>A user with the <code>Namespace Admin</code> role is authorized to perform the following actions within the namespace:</p> <ol> <li>Create and manage all <code>ManagedClusters</code> in the namespace</li> <li>Create and manage <code>Cluster</code> and <code>Service Templates</code> in the namespace</li> <li>Manage the distribution and upgrade sequences of Templates within the namespace</li> <li>Create and manage Flux <code>HelmRepositories</code> and <code>HelmCharts</code> in the namespace</li> <li>Manage <code>Credentials</code> created by any user in the namespace</li> </ol>"},{"location":"rbac/roles/#namespace-editor","title":"Namespace Editor","text":"<p>The <code>Namespace Editor</code> role allows users to create and modify <code>ManagedClusters</code> within namespace using predefined <code>Credentials</code> and <code>Templates</code>.</p> <p>Name: <code>hmc-namespace-editor-role</code></p> <p>Aggregation Rule: Includes all <code>ClusterRoles</code> with the labels:</p> <ul> <li><code>hmc.mirantis.com/aggregate-to-namespace-editor: true</code></li> </ul> <p>Permissions:</p> <ol> <li>Full access to <code>ManagedClusters</code> in the allowed namespace</li> <li>Read access to <code>Credentials</code>, <code>Cluster</code> and <code>Service Templates</code>, and <code>TemplateChains</code> in the namespace</li> <li>Read access to Flux <code>HelmRepositories</code> and <code>HelmCharts</code> in the namespace</li> </ol> <p>Use case</p> <p>A user with the <code>Namespace Editor</code> role has the following permissions in the namespace:</p> <ol> <li>Can create and manage <code>ManagedCluster</code> objects in the namespace using existing <code>Credentials</code> and <code>Templates</code></li> <li>Can list and view detailed information about the <code>Credentials</code> available in the namespace</li> <li>Can list and view detailed information about the available <code>Cluster</code> and <code>Service Templates</code> and the <code>Templates'</code>    upgrade sequences</li> <li>Can list and view detailed information about the Flux <code>HelmRepositories</code> and <code>HelmCharts</code></li> </ol>"},{"location":"rbac/roles/#namespace-viewer","title":"Namespace Viewer","text":"<p>The <code>Namespace Viewer</code> role grants read-only access to resources within a namespace.</p> <p>Name: <code>hmc-namespace-viewer-role</code></p> <p>Aggregation Rule: Includes all <code>ClusterRoles</code> with the labels:</p> <ul> <li><code>hmc.mirantis.com/aggregate-to-namespace-viewer: true</code></li> </ul> <p>Permissions:</p> <ol> <li>Read access to <code>ManagedClusters</code> in the namespace</li> <li>Read access to <code>Credentials</code>, <code>Cluster</code> and <code>Service Templates</code>, and <code>TemplateChains</code> in the namespace</li> <li>Read access to Flux <code>HelmRepositories</code> and <code>HelmCharts</code> in the namespace</li> </ol> <p>Use case</p> <p>A user with the <code>Namespace Viewer</code> role has the following permissions in the namespace:</p> <ol> <li>Can list and view detailed information about all the <code>ManagedCluster</code> objects in the allowed namespace</li> <li>Can list and view detailed information about <code>Credentials</code> available in the specific namespace</li> <li>Can list and view detailed information about available <code>Cluster</code> and <code>Service Templates</code> and the <code>Templates'</code>    upgrade sequences</li> <li>Can list and view detailed information about Flux <code>HelmRepositories</code> and <code>HelmCharts</code></li> </ol>"},{"location":"template/byo-templates/","title":"Bring your own Templates","text":"<p>Here are the instructions on how to bring your own Template to HMC:</p> <ol> <li>Create a HelmRepository object containing the URL to the external Helm repository. Label it with <code>hmc.mirantis.com/managed: \"true\"</code>.</li> <li>Create a HelmChart object referencing the <code>HelmRepository</code> as a <code>sourceRef</code>, specifying the name and version of your Helm chart. Label it with <code>hmc.mirantis.com/managed: \"true\"</code>.</li> <li>Create a <code>ClusterTemplate</code>, <code>ServiceTemplate</code> or <code>ProviderTemplate</code> object referencing this helm chart in <code>.spec.helm.chartRef</code>. <code>chartRef</code> is a field of the CrossNamespaceSourceReference kind. For <code>ClusterTemplate</code> and <code>ServiceTemplate</code> configure the namespace where this template should reside (<code>metadata.namespace</code>).</li> </ol> <p>Note</p> <p> <code>ClusterTemplate</code> and <code>ServiceTemplate</code> objects should reside in the same namespace as the <code>ManagedCluster</code> referencing them. The <code>ManagedCluster</code> can't reference the Template from another namespace (the creation request will be declined by the admission webhook). All <code>ClusterTemplates</code> and <code>ServiceTemplates</code> shipped with HMC reside in the system namespace (defaults to <code>hmc-system</code>). To get the instructions on how to distribute Templates along multiple namespaces, read Template Life Cycle Management.</p> <p>Here is an example of a custom <code>ClusterTemplate</code> with the <code>HelmChart</code> reference:</p> <pre><code>apiVersion: source.toolkit.fluxcd.io/v1\nkind: HelmRepository\nmetadata:\n  name: custom-templates-repo\n  namespace: hmc-system\n  labels:\n    hmc.mirantis.com/managed: \"true\"\nspec:\n  insecure: true\n  interval: 10m0s\n  provider: generic\n  type: oci\n  url: oci://ghcr.io/external-templates-repo/charts\n</code></pre> <pre><code>apiVersion: source.toolkit.fluxcd.io/v1\nkind: HelmChart\nmetadata:\n  name: custom-template-chart\n  namespace: hmc-system\n  labels:\n    hmc.mirantis.com/managed: \"true\"\nspec:\n  interval: 5m0s\n  chart: custom-template-chart-name\n  reconcileStrategy: ChartVersion\n  sourceRef:\n    kind: HelmRepository\n    name: custom-templates-repo\n  version: 0.2.0\n</code></pre> <pre><code>apiVersion: hmc.mirantis.com/v1alpha1\nkind: ClusterTemplate\nmetadata:\n  name: os-k0smotron\n  namespace: hmc-system\nspec:\n  providers:\n    - bootstrap-k0smotron\n    - control-plane-k0smotron\n    - infrastructure-openstack\n  helm:\n    chartRef:\n      kind: HelmChart\n      name: custom-template-chart\n      namespace: default\n</code></pre> <p>The <code>*Template</code> should follow the rules mentioned below:</p> <p><code>.spec.providers</code> should contain the list of required Cluster API providers: <code>infrastructure</code>, <code>bootstrap</code> and <code>control-plane</code>. As an alternative, the referenced helm chart may contain the specific annotations in the <code>Chart.yaml</code> (value is a list of providers divided by comma). These fields are only used for validation. For example:</p> <p><code>ClusterTemplate</code> spec:</p> <pre><code>spec:\n  providers:\n  - bootstrap-k0smotron\n  - control-plane-k0smotron\n  - infrastructure-aws\n</code></pre> <p><code>Chart.yaml</code>:</p> <pre><code>annotations:\n  cluster.x-k8s.io/provider: infrastructure-aws, control-plane-k0smotron, bootstrap-k0smotron\n</code></pre>"},{"location":"template/byo-templates/#compatibility-attributes","title":"Compatibility attributes","text":"<p>Each of the <code>*Template</code> resources has compatibility versions attributes to constraint the core <code>CAPI</code>, <code>CAPI</code> provider or Kubernetes versions. CAPI-related version constraints must be set in the <code>CAPI</code> contract format. Kubernetes version constraints must be set in the Semantic Version format. Each attribute can be set either via the corresponding <code>.spec</code> fields or via the annotations. Values set via the <code>.spec</code> have precedence over the values set via the annotations.</p> <p>Note</p> <p> All of the compatibility attributes are optional, and validation checks only take place if both of the corresponding type attributes (e.g. provider contract versions in both <code>ProviderTemplate</code> and <code>ClusterTemplate</code>) are set.</p> <ol> <li> <p>The <code>ProviderTemplate</code> resource has dedicated fields to set compatible <code>CAPI</code> contract versions along with CRDs contract versions supported by the provider. Given contract versions will be then set accordingly in the <code>.status</code> field. Compatibility contract versions are key-value pairs, where the key is the core <code>CAPI</code> contract version, and the value is an underscore-delimited (_) list of provider contract versions supported by the core <code>CAPI</code>. For the core <code>CAPI</code> Template values should be empty.</p> <p>Example with the <code>.spec</code>:</p> <pre><code>apiVersion: hmc.mirantis.com/v1alpha1\nkind: ProviderTemplate\n# ...\nspec:\n  providers:\n  - infrastructure-aws\n  capiContracts:\n    # commented is the example exclusively for the core CAPI Template\n    # v1alpha3: \"\"\n    # v1alpha4: \"\"\n    # v1beta1: \"\"\n    v1alpha3: v1alpha3\n    v1alpha4: v1alpha4\n    v1beta1: v1beta1_v1beta2\n</code></pre> <p>Example with the <code>annotations</code> in the <code>Chart.yaml</code> with the same logic as in the <code>.spec</code>:</p> <pre><code>annotations:\n  cluster.x-k8s.io/provider: infrastructure-aws\n  cluster.x-k8s.io/v1alpha3: v1alpha3\n  cluster.x-k8s.io/v1alpha4: v1alpha4\n  cluster.x-k8s.io/v1beta1: v1beta1_v1beta2\n</code></pre> </li> <li> <p>The <code>ClusterTemplate</code> resource has dedicated fields to set an exact compatible Kubernetes version in the Semantic Version format and required contract versions per each provider to match against the related <code>ProviderTemplate</code> objects. Given compatibility attributes will be then set accordingly in the <code>.status</code> field. Compatibility contract versions are key-value pairs, where the key is the name of the provider, and the value is the provider contract version required to be supported by the provider.</p> <p>Example with the <code>.spec</code>:</p> <pre><code>apiVersion: hmc.mirantis.com/v1alpha1\nkind: ClusterTemplate\n# ...\nspec:\n  k8sVersion: 1.30.0 # only exact semantic version is applicable\n  providers:\n  - bootstrap-k0smotron\n  - control-plane-k0smotron\n  - infrastructure-aws\n  providerContracts:\n    bootstrap-k0smotron: v1beta1 # only a single contract version is applicable\n    control-plane-k0smotron: v1beta1\n    infrastructure-aws: v1beta2\n</code></pre> <p>Example with the <code>.annotations</code> in the <code>Chart.yaml</code>:</p> <pre><code>annotations:\n  cluster.x-k8s.io/provider: infrastructure-aws, control-plane-k0smotron, bootstrap-k0smotron\n  cluster.x-k8s.io/bootstrap-k0smotron: v1beta1\n  cluster.x-k8s.io/control-plane-k0smotron: v1beta1\n  cluster.x-k8s.io/infrastructure-aws: v1beta2\n  hmc.mirantis.com/k8s-version: 1.30.0\n</code></pre> </li> <li> <p>The <code>ServiceTemplate</code> resource has dedicated fields to set an compatibility constrained Kubernetes version to match against the related <code>ClusterTemplate</code> objects. Given compatibility values will be then set accordingly in the <code>.status</code> field.</p> <p>Example with the <code>.spec</code>:</p> <pre><code>apiVersion: hmc.mirantis.com/v1alpha1\nkind: ServiceTemplate\n# ...\nspec:\n  k8sConstraint: \"^1.30.0\" # only semantic version constraints are applicable\n</code></pre> <p>Example with the <code>annotations</code> in the <code>Chart.yaml</code>:</p> <pre><code>hmc.mirantis.com/k8s-version-constraint: ^1.30.0\n</code></pre> </li> </ol>"},{"location":"template/byo-templates/#compatibility-attributes-enforcement","title":"Compatibility attributes enforcement","text":"<p>The aforedescribed attributes are being checked sticking to the following rules:</p> <ul> <li>both the exact and constraint version of the same type (e.g. <code>k8sVersion</code> and <code>k8sConstraint</code>) must be set otherwise no check is performed;</li> <li>if a <code>ClusterTemplate</code> object's providers contract version does not satisfy contract versions from the related <code>ProviderTemplate</code> object, the updates to the <code>ManagedCluster</code> object will be blocked;</li> <li>if a <code>ProviderTemplate</code> object's <code>CAPI</code> contract version (e.g. in a <code>v1beta1: v1beta1_v1beta2</code> key-value pair, the key <code>v1beta1</code> is the core <code>CAPI</code> contract version) is not listed in the core <code>CAPI</code> <code>ProviderTemplate</code> object, the updates to the <code>Management</code> object will be blocked;</li> <li>if a <code>ClusterTemplate</code> object's exact kubernetes version does not satisfy the kubernetes version constraint from the related <code>ServiceTemplate</code> object, the updates to the <code>ManagedCluster</code> object will be blocked.</li> </ul>"},{"location":"template/byo-templates/#remove-templates-shipped-with-hmc","title":"Remove Templates shipped with HMC","text":"<p>If you need to limit the templates that exist in your HMC installation, follow the instructions below:</p> <ol> <li> <p>Get the list of <code>ProviderTemplates</code>, <code>ClusterTemplates</code> or <code>ServiceTemplates</code> shipped with HMC. For example, for <code>ClusterTemplate</code> objects, run:</p> <pre><code>kubectl get clustertemplates -n hmc-system -l helm.toolkit.fluxcd.io/name=hmc-templates\n</code></pre> <p>Example output:</p> <pre><code>NAME                       VALID\naws-hosted-cp              true\naws-standalone-cp          true\n</code></pre> </li> <li> <p>Remove the template from the list using <code>kubectl delete</code>. For example:</p> <pre><code>kubectl delete clustertemplate -n hmc-system &lt;template-name&gt;\n</code></pre> </li> </ol>"},{"location":"template/main/","title":"Templates system","text":"<p>By default, 2A delivers a set of default <code>ProviderTemplate</code>, <code>ClusterTemplate</code> and <code>SystemTemplate</code> objects:</p> <ul> <li><code>ProviderTemplate</code>    The template containing the configuration of the provider, ex. k0smotron. Cluster-scoped.</li> <li><code>ClusterTemplate</code>    The template containing the configuration of the cluster objects. Namespace-scoped.</li> <li><code>ServiceTemplate</code>    The template containing the configuration of the service to be installed on the managed cluster. Namespace-scoped.</li> </ul> <p>All Templates are immutable. You can also build your own templates and use them for deployment along with the Templates shipped with 2A.</p>"},{"location":"template/main/#template-life-cycle-management","title":"Template Life Cycle Management","text":"<p>Cluster and Service Templates can be delivered to target namespaces using the <code>TemplateManagement</code>, <code>ClusterTemplateChain</code> and <code>ServiceTemplateChain</code> objects. <code>TemplateManagement</code> object contains the list of access rules to apply. Each access rule contains the namespaces' definition to deliver templates into and the template chains. Each <code>ClusterTemplateChain</code> and <code>ServiceTemplateChain</code> contains the supported templates and the upgrade sequences for them.</p> <p>The example of the Cluster Template Management:</p> <ol> <li>Create <code>ClusterTemplateChain</code> object in the system namespace (defaults to <code>hmc-system</code>). Properly configure    the list of <code>.spec.supportedTemplates[].availableUpgrades</code> for the specified <code>ClusterTemplate</code> if the upgrade is allowed. For example:</li> </ol> <pre><code>apiVersion: hmc.mirantis.com/v1alpha1\nkind: ClusterTemplateChain\nmetadata:\n  name: aws\n  namespace: hmc-system\nspec:\n  supportedTemplates:\n    - name: aws-standalone-cp-0-0-1\n      availableUpgrades:\n        - name: aws-standalone-cp-0-0-2\n    - name: aws-standalone-cp-0-0-2\n</code></pre> <ol> <li>Edit <code>TemplateManagement</code> object and configure the <code>.spec.accessRules</code>.    For example, to apply all templates and upgrade sequences defined in the <code>aws</code> <code>ClusterTemplateChain</code> to the    <code>default</code> namespace, the following <code>accessRule</code> should be added:</li> </ol> <pre><code>spec:\n  accessRules:\n  - targetNamespaces:\n      list:\n        - default\n    clusterTemplateChains:\n      - aws\n</code></pre> <p>The HMC controllers will deliver all the <code>ClusterTemplate</code> objects across the target namespaces. As a result, the new objects should be created:</p> <ul> <li><code>ClusterTemplateChain</code> <code>default/aws</code></li> <li><code>ClusterTemplate</code> <code>default/aws-standalone-cp-0-0-1</code></li> <li><code>ClusterTemplate</code> <code>default/aws-standalone-cp-0-0-2</code> (available for the upgrade from <code>aws-standalone-cp-0-0-1</code>)</li> </ul> <p>Note</p> <ol> <li>The target <code>ClusterTemplate</code> defined as the available for the upgrade should reference the same helm chart name as the source <code>ClusterTemplate</code>. Otherwise, after the upgrade is triggered, the cluster will be removed and then, recreated from scratch even if the objects in the helm chart are the same.</li> <li>The target template should not affect immutable fields or any other incompatible internal objects upgrades, otherwise the upgrade will fail.</li> </ol>"},{"location":"usage/2a-upgrade/","title":"Upgrading 2A to the newer version","text":"<p>Note</p> <p>To upgrade 2A the user must have <code>Global Admin</code> role. For the detailed information about 2A RBAC, refer to the RBAC documentation.</p> <p>Follow the steps below to update 2A to a newer version:</p> <p>Step 1. Create a New <code>Release</code> Object</p> <p>Create a <code>Release</code> object in the management cluster for the desired version. For example, to create a <code>Release</code> for version <code>v0.0.4</code>, run the following command:</p> <pre><code>VERSION=v0.0.4\nkubectl create -f https://github.com/Mirantis/hmc/releases/download/${VERSION}/release.yaml\n</code></pre> <p>Step 2. Update the <code>Management</code> Object with the New <code>Release</code></p> <ul> <li>List available <code>Releases</code>:</li> </ul> <p>To view all available <code>Releases</code>, run:</p> <pre><code>kubectl get releases\n</code></pre> <p>Example output:</p> <pre><code>NAME        AGE\nhmc-0-0-3   71m\nhmc-0-0-4   65m\n</code></pre> <ul> <li>Patch the <code>Management</code> Object with the New <code>Release</code> Name:</li> </ul> <p>Update the <code>spec.release</code> field in the <code>Management</code> object to point to the new release. Replace <code>hmc-0-0-4</code> with the name of your new release:</p> <pre><code>RELEASE_NAME=hmc-0-0-4\nkubectl patch management.hmc hmc --patch \"{\\\"spec\\\":{\\\"release\\\":\\\"${RELEASE_NAME}\\\"}}\" --type=merge\n</code></pre> <p>Step 3. Verify the Upgrade</p> <p>Check the status of the <code>Management</code> object to monitor the readiness of the components:</p> <pre><code>kubectl get management.hmc hmc -o=jsonpath={.status} | jq\n</code></pre>"},{"location":"usage/airgap/","title":"Air-gapped Installation Guide","text":""},{"location":"usage/airgap/#prerequisites","title":"Prerequisites","text":"<p>In order to install HMC in an air-gapped environment, you need will need the following:</p> <ul> <li>An installed k0s cluster that will be used as the management cluster.  If you   do not yet have a k0s cluster, you can follow the Airgapped Installation   documentation.  k0s is recommended for airgapped installations because it   implements an OCI image bundle watcher which allows k0s to utilize a bundle   of management cluster images easily. Any Kubernetes distribution can be   used, but instructions for using k0s are provided here.</li> <li>The <code>KUBECONFIG</code> of a management cluster that will be the target for the HMC   installation.</li> <li> <p>A registry that is accessible from the airgapped hosts to store the HMC images.   If you do not have a registry you can deploy a local Docker registry   or use mindthegap</p> <p>Warning</p> <p> If using a local Docker registry, ensure the registry URL is added to the <code>insecure-registries</code> key within the Docker <code>/etc/docker/daemon.json</code> file. <pre><code>{\n  \"insecure-registries\": [\"&lt;registry-url&gt;\"]\n}\n</code></pre></p> </li> <li> <p>A registry and associated chart repository for hosting HMC charts.  At this   time all HMC charts MUST be hosted in a single OCI chart repository.  See   Use OCI-based registries in the   Helm documentation for more information.</p> </li> <li>jq, Helm and Docker binaries   installed on the machine where the <code>airgap-push.sh</code> script will be run.</li> </ul>"},{"location":"usage/airgap/#installation","title":"Installation","text":"<ol> <li> <p>Download the HMC airgap bundle, the bundle contains the following:</p> <ul> <li><code>images/hmc-images-&lt;version&gt;.tgz</code> - The image bundle tarball for the   management cluster, this bundle will be loaded into the management   cluster.</li> <li><code>images/hmc-extension-images-&lt;version&gt;.tgz</code> - The image bundle tarball for   the managed clusters, this bundle will be pushed to a registry where the   images can be accessed by the managed clusters.</li> <li><code>charts</code> - Contains the HMC Helm chart, dependency charts and k0s   extensions charts within the <code>extensions</code> directory.  All of these charts   will be pushed to a chart repository within a registry.</li> <li><code>scripts/airgap-push.sh</code> - A script that will aid in re-tagging and   pushing the <code>ManagedCluster</code> required charts and images to a desired   registry.</li> </ul> </li> <li> <p>Extract and use the <code>airgap-push.sh</code> script to push the <code>extensions</code> images    and <code>charts</code> contents to the registry.  Ensure you have logged into the    registry using both <code>docker login</code> and <code>helm registry login</code> before running    the script.</p> <pre><code>tar xvf hmc-airgap-&lt;version&gt;.tgz scripts/airgap-push.sh\n./scripts/airgap-push.sh -r &lt;registry&gt; -c &lt;chart-repo&gt; -a hmc-airgap-&lt;version&gt;.tgz\n</code></pre> </li> <li> <p>Next, extract the <code>management</code> bundle tarball and sync the images to the    k0s cluster which will host the management cluster.  See Sync the Bundle File    for more information.</p> <p>Note</p> <p> Multiple image bundles can be placed in the <code>/var/lib/k0s/images</code> directory for k0s to use and the existing <code>k0s</code> airgap bundle does not need to be merged into the <code>hmc-images-&lt;version&gt;.tgz</code> bundle.</p> <pre><code>tar -C /var/lib/k0s -xvf hmc-airgap-&lt;version&gt;.tgz \"images/hmc-images-&lt;version&gt;.tgz\"\n</code></pre> </li> <li> <p>Install the HMC Helm chart on the management cluster from the registry where    the HMC charts were pushed.  The HMC controller image is loaded as part of    the airgap <code>management</code> bundle and does not need to be customized within the    Helm chart, but the default chart repository configured via    <code>controller.defaultRegistryURL</code> should be set to reference the repository    where charts have been pushed.</p> <pre><code>helm install hmc oci://&lt;chart-repository&gt;/hmc \\\n  --version &lt;hmc-version&gt; \\\n  -n hmc-system \\\n  --create-namespace \\\n  --set controller.defaultRegistryURL=\"oci://&lt;chart-repository&gt;\"\n</code></pre> </li> <li> <p>Within the <code>spec:</code> for your desired <code>ManagedCluster</code> object, specify the    custom image registry and chart repository to be used (the registry and chart    repository where the <code>extensions</code> bundle and charts were pushed).</p> <pre><code>spec:\n config:\n   extensions:\n    imageRepository: ${IMAGE_REPOSITORY}\n    chartRepository: ${CHART_REPOSITORY}\n</code></pre> </li> </ol>"},{"location":"usage/cluster-update/","title":"Managed Cluster update","text":"<p>To update the <code>ManagedCluster</code>, update <code>.spec.template</code> in the <code>ManagedCluster</code> object to the new <code>ClusterTemplate</code> name:</p> <p>Run:</p> <pre><code>kubectl patch managedcluster.hmc &lt;cluster-name&gt; -n &lt;namespace&gt; --patch '{\"spec\":{\"template\":\"&lt;new-template-name&gt;\"}}' --type=merge\n</code></pre> <p>Then, check the status of the <code>ManagedCluster</code> object:</p> <pre><code>kubectl get managedcluster.hmc &lt;cluster-name&gt; -n &lt;namespace&gt;\n</code></pre> <p>In the commands above, replace the parameters enclosed in angle brackets with the corresponding values.</p> <p>To get more details, run the previous command with the <code>-o=yaml</code> option and check the <code>.status.conditions</code>.</p> <p>Note</p> <p> The <code>ManagedCluster</code> is allowed to be updated to specific templates only. The templates available for the update are defined in the <code>ClusterTemplateChain</code> objects. Also, the <code>TemplateManagement</code> object should contain properly configured <code>spec.accessRules</code> with the list of <code>ClusterTemplateChain</code> object names and the namespaces where the supported templates from the chain spec will be delivered. For details, see: Template Life Cycle Management.</p>"},{"location":"usage/create-managed-cluster/","title":"Create Managed Cluster","text":""},{"location":"usage/create-managed-cluster/#creation-process","title":"Creation Process","text":""},{"location":"usage/create-managed-cluster/#step-1-create-credential","title":"Step 1: Create Credential","text":"<ul> <li>Create a <code>Credential</code> object with all credentials required per the   Credential System.</li> </ul>"},{"location":"usage/create-managed-cluster/#step-2-select-the-template","title":"Step 2: Select the Template","text":"<p>For details about the templates in Project 2A, see the Templates system.</p> <ul> <li>Set the <code>KUBECONFIG</code> environment variable to the path to the management   cluster kubeconfig file. Then select the <code>Template</code> you want to use for the   deployment. To list all available templates, run:</li> </ul> <pre><code>kubectl get clustertemplate -n hmc-system\n</code></pre> <p>Note</p> <p> If you want to deploy a hosted control plane template, check additional notes on hosted control planes for each of the clustertemplate sections:</p> <ul> <li>AWS Hosted Control Plane</li> <li>vSphere Hosted Control Plane</li> </ul>"},{"location":"usage/create-managed-cluster/#step-3-create-the-managedcluster-object-yaml-configuration","title":"Step 3: Create the ManagedCluster Object YAML Configuration","text":"<ul> <li> <p>Create the file with the <code>ManagedCluster</code> configuration:</p> <pre><code>apiVersion: hmc.mirantis.com/v1alpha1\nkind: ManagedCluster\nmetadata:\n  name: &lt;cluster-name&gt;\n  namespace: &lt;hmc-system-namespace&gt;\nspec:\n  template: &lt;template-name&gt;\n  credential: &lt;infrastructure-provider-credential-name&gt;\n  dryRun: &lt;\"true\" or \"false\": defaults to \"false\"&gt;\n  config:\n    &lt;cluster-configuration&gt;\n</code></pre> </li> </ul> <p>Note</p> <p> Substitute the parameters enclosed in angle brackets with the corresponding values. Enable the <code>dryRun</code> flag if required. For details, see Dry Run.</p> <p>Following is an interpolated example.</p> <p>Example</p> <p><code>ManagedCluster</code> for AWS Infrastructure Provider Object Example</p> <pre><code>apiVersion: hmc.mirantis.com/v1alpha1\nkind: ManagedCluster\nmetadata:\n  name: my-managed-cluster\n  namespace: hmc-system\nspec:\n  template: aws-standalone-cp-0-0-2\n  credential: aws-credential\n  dryRun: true\n  config:\n    region: us-west-2\n    controlPlane:\n      instanceType: t3.small\n    worker:\n      instanceType: t3.small\n</code></pre>"},{"location":"usage/create-managed-cluster/#step-4-apply-the-managedcluster-configuration-to-create-it","title":"Step 4: Apply the <code>ManagedCluster</code> Configuration to Create it","text":"<ul> <li> <p>Apply the <code>ManagedCluster</code> object to your Project 2A deployment:</p> <pre><code>kubectl apply -f managedcluster.yaml\n</code></pre> </li> </ul>"},{"location":"usage/create-managed-cluster/#step-5-check-the-status-of-the-managedcluster-object","title":"Step 5: Check the Status of the <code>ManagedCluster</code> Object","text":"<ul> <li> <p>Check the status of the newly created <code>ManagedCluster</code>:</p> <pre><code>kubectl -n &lt;namespace&gt; get managedcluster.hmc &lt;cluster-name&gt; -o=yaml\n</code></pre> </li> </ul> <p>Info</p> <p> Reminder: <code>&lt;namespace&gt;</code> and <code>&lt;cluster-name&gt;</code> are defined in the <code>.metadata</code> section of the <code>ManagedCluster</code> object you created above.</p>"},{"location":"usage/create-managed-cluster/#step-6-wait-for-infrastructure-and-cluster-to-be-provisioned","title":"Step 6: Wait for Infrastructure and Cluster to be Provisioned","text":"<ul> <li> <p>Wait for infrastructure to be provisioned and the cluster to be deployed:</p> <pre><code>kubectl -n &lt;namespace&gt; get cluster &lt;cluster-name&gt; -o=yaml\n</code></pre> </li> </ul> <p>Tip</p> <p> You may also watch the process with the <code>clusterctl describe</code> command (requires the <code>clusterctl</code> CLI to be installed):</p> <pre><code>clusterctl describe cluster &lt;cluster-name&gt; -n &lt;namespace&gt; --show-conditions all\n</code></pre>"},{"location":"usage/create-managed-cluster/#step-7-retrieve-kubernetes-configuration-of-your-managed-cluster","title":"Step 7: Retrieve Kubernetes Configuration of Your Managed Cluster","text":"<ul> <li> <p>Retrieve the Kubernetes configuration of your managed cluster when it is   finished provisioning:</p> <pre><code>kubectl get secret -n &lt;namespace&gt; &lt;cluster-name&gt;-kubeconfig -o=jsonpath={.data.value} | base64 -d &gt; kubeconfig\n</code></pre> </li> </ul>"},{"location":"usage/create-managed-cluster/#dry-run","title":"Dry Run","text":"<p>Project 2A <code>ManagedCluster</code> supports two modes: with and without <code>.spec.dryRun</code> (defaults to <code>false</code>).</p> <p>If no configuration (<code>.spec.config</code>) is specified, the <code>ManagedCluster</code> object will be populated with defaults (default configuration can be found in the corresponding <code>Template</code> status) and automatically have <code>.spec.dryRun</code> set to <code>true</code>.</p> <p>Example</p> <p><code>ManagedCluster</code> with default configuration</p> <pre><code>apiVersion: hmc.mirantis.com/v1alpha1\nkind: ManagedCluster\nmetadata:\n  name: my-managed-cluster\n  namespace: hmc-system\nspec:\n  config:\n    clusterNetwork:\n      pods:\n        cidrBlocks:\n        - 10.244.0.0/16\n      services:\n        cidrBlocks:\n        - 10.96.0.0/12\n    controlPlane:\n      iamInstanceProfile: control-plane.cluster-api-provider-aws.sigs.k8s.io\n      instanceType: \"\"\n    controlPlaneNumber: 3\n    k0s:\n      version: v1.27.2+k0s.0\n    publicIP: false\n    region: \"\"\n    sshKeyName: \"\"\n    worker:\n      amiID: \"\"\n      iamInstanceProfile: nodes.cluster-api-provider-aws.sigs.k8s.io\n      instanceType: \"\"\n    workersNumber: 2\n  template: aws-standalone-cp-0-0-2\n  credential: aws-credential\n  dryRun: true\n</code></pre> <p>After you adjust your configuration and ensure that it passes validation (<code>TemplateReady</code> condition from <code>.status.conditions</code>), remove the <code>.spec.dryRun</code> flag to proceed with the deployment.</p> <p>Here is an example of a <code>ManagedCluster</code> object that passed the validation:</p> <p>Example</p> <p><code>ManagedCluster</code> object that passed the validation</p> <pre><code>apiVersion: hmc.mirantis.com/v1alpha1\nkind: ManagedCluster\nmetadata:\n  name: my-managed-cluster\n  namespace: hmc-system\nspec:\n  template: aws-standalone-cp-0-0-2\n  credential: aws-credential\n  config:\n    region: us-east-2\n    publicIP: true\n    controlPlaneNumber: 1\n    workersNumber: 1\n    controlPlane:\n      instanceType: t3.small\n    worker:\n      instanceType: t3.small\n  status:\n    conditions:\n    - lastTransitionTime: \"2024-07-22T09:25:49Z\"\n      message: Template is valid\n      reason: Succeeded\n      status: \"True\"\n      type: TemplateReady\n    - lastTransitionTime: \"2024-07-22T09:25:49Z\"\n      message: Helm chart is valid\n      reason: Succeeded\n      status: \"True\"\n      type: HelmChartReady\n    - lastTransitionTime: \"2024-07-22T09:25:49Z\"\n      message: ManagedCluster is ready\n      reason: Succeeded\n      status: \"True\"\n      type: Ready\n    observedGeneration: 1\n</code></pre>"},{"location":"usage/create-managed-cluster/#cleanup","title":"Cleanup","text":"<ol> <li> <p>Remove the Management object:</p> <pre><code>kubectl delete management.hmc hmc\n</code></pre> </li> </ol> <p>Note</p> <p> Ensure you have no Project 2A <code>ManagedCluster</code> objects left in the cluster prior to Management deletion.</p> <ol> <li> <p>Remove the <code>hmc</code> Helm release:</p> <pre><code>helm uninstall hmc -n hmc-system\n</code></pre> </li> <li> <p>Remove the <code>hmc-system</code> namespace:</p> <pre><code>kubectl delete ns hmc-system\n</code></pre> </li> </ol>"},{"location":"usage/installation/","title":"Installation Guide","text":"<p>This section describes how to install Project 2A.</p>"},{"location":"usage/installation/#tldr","title":"TL;DR","text":"<pre><code>export KUBECONFIG=&lt;path-to-management-kubeconfig&gt;\n</code></pre> <pre><code>helm install hmc oci://ghcr.io/mirantis/hmc/charts/hmc --version &lt;hmc-version&gt; -n hmc-system --create-namespace\n</code></pre> <p>This will use the defaults as seen in Extended Management Configuration section below.</p>"},{"location":"usage/installation/#finding-releases","title":"Finding Releases","text":"<p>Releases are tagged in the GitHub repository and can be found here.</p>"},{"location":"usage/installation/#extended-management-configuration","title":"Extended Management Configuration","text":"<p>Project 2A is deployed with the following default configuration, which may vary depending on the release version:</p> <p><pre><code>apiVersion: hmc.mirantis.com/v1alpha1\nkind: Management\nmetadata:\n  name: hmc\nspec:\n  providers:\n  - name: k0smotron\n  - name: cluster-api-provider-aws\n  - name: cluster-api-provider-azure\n  - name: cluster-api-provider-vsphere\n  - name: projectsveltos\n  release: hmc-0-0-3\n</code></pre> To see what is included in a specific release, look at the <code>release.yaml</code> file in the tagged release. For example, here is the v0.0.3 release.yaml.</p> <p>There are two options to override the default management configuration of Project 2A:</p> <ol> <li> <p>Update the <code>Management</code> object after the Project 2A installation using <code>kubectl</code>:</p> <p><code>kubectl --kubeconfig &lt;path-to-management-kubeconfig&gt; edit management</code></p> </li> <li> <p>Deploy 2A skipping the default <code>Management</code> object creation and provide your    own <code>Management</code> configuration:</p> <ul> <li>Create <code>management.yaml</code> file and configure core components and providers.</li> <li> <p>Specify <code>--create-management=false</code> controller argument and install Project 2A:   If installing using <code>helm</code> add the following parameter to the <code>helm   install</code> command:</p> <p><code>--set=\"controller.createManagement=false\"</code></p> </li> <li> <p>Create <code>hmc</code> <code>Management</code> object after Project 2A installation:</p> <pre><code>kubectl --kubeconfig &lt;path-to-management-kubeconfig&gt; create -f management.yaml\n</code></pre> </li> </ul> </li> </ol>"},{"location":"usage/installation/#air-gapped-installation","title":"Air-gapped installation","text":"<p>Follow the Air-gapped Installation Guide to get the instructions on how to perform 2A installation in the air-gapped environment.</p>"},{"location":"usage/installation/#cleanup","title":"Cleanup","text":"<ol> <li>Remove the Management object:</li> </ol> <pre><code>  kubectl delete management.hmc hmc\n</code></pre> <p>Warning</p> <p>Make sure you have no Project 2A <code>ManagedCluster</code> objects left in the cluster prior to deletion.</p> <ol> <li>Remove the <code>hmc</code> Helm release:</li> </ol> <pre><code>  helm uninstall hmc -n hmc-system\n</code></pre> <ol> <li>Remove the <code>hmc-system</code> namespace:</li> </ol> <pre><code>  kubectl delete ns hmc-system\n</code></pre>"}]}